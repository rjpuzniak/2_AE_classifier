{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torchio as tio\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchio as tio\n",
    "from torchio.transforms import (RescaleIntensity,RandomFlip,Compose, HistogramStandardization, RandomAffine, RandomNoise, ToCanonical)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# CHIASM dobry\\n# ABIDE dobry\\n# Athletes dobry\\n# COBRE dobry\\n# HCP dobry\\n# Leipzig good\\n# MCIC złe\\n# UoN dobry\\n\\n# Splits\\ntrain_split = 0.85\\ndev_split = 0.15\\ntest_split = 0.0\\n\\n# Dictionary with study design\\ndesign = {}\\n\\ndesign['train']={}\\ndesign['dev_train']={}\\ndesign['dev_test']={}\\ndesign['test']={}\\n\\ndesign['train']['all']={}\\ndesign['dev_train']['all']={}\\ndesign['dev_test']['all']={}\\ndesign['test']['all']={}\\n\\n# Training data\\nfor group in train_groups:\\n    \\n    #design['dev_train'][group]={}\\n    #design['test'][group]={}\\n\\n    # Idices of all subjects\\n    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \\n    # Randomize order\\n    random.shuffle(ids) \\n    # Find split ratios\\n    train_idx = np.int(np.floor(len(ids)*train_split))\\n    dev_idx = np.int(np.floor(len(ids)*(train_split+dev_split)))\\n    \\n    for i in range(len(ids)):\\n        \\n        path_to_folder='../../1_Data/1_Input/'+group+'/'+ids[i]+'/'\\n        \\n        files={}\\n        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\\n        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\\n        files['chiasm']=path_to_folder+'chiasm.nii.gz'\\n    \\n        if i+1<=train_idx:\\n            design['train']['all'][ids[i]]=files\\n        elif i+1 > dev_idx:\\n            design['test']['all'][ids[i]]=files\\n            #design['test'][group][ids[i]]=files\\n        else:\\n            design['dev_train']['all'][ids[i]]=files\\n            #design['dev_train'][group][ids[i]]=files\\n        \\n# Dev data\\nfor group in test_dev_groups:\\n\\n    # Idices of all subjects\\n    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \\n    \\n    for sub_id in ids:\\n        \\n        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\\n        \\n        files={}\\n        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\\n        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\\n        files['chiasm']=path_to_folder+'chiasm.nii.gz'\\n        \\n        design['dev_test']['all'][sub_id]=files\\n    \\n# Test data\\nfor group in test_groups:\\n    \\n    #design['test'][group]={}\\n\\n    # Idices of all subjects\\n    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \\n    \\n    for sub_id in ids:\\n        \\n        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\\n        \\n        files={}\\n        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\\n        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\\n        files['chiasm']=path_to_folder+'chiasm.nii.gz'\\n        \\n        design['test']['all'][sub_id]=files \\n        #design['test'][group][sub_id]=files  \\n        \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dictionary describing assignment of participants to the groups\n",
    "\n",
    "# Groups\n",
    "train_groups=['ABIDE','Athletes','HCP','COBRE','Leipzig']\n",
    "test_dev_groups=['UoN']\n",
    "test_groups=['CHIASM']\n",
    "'''\n",
    "# CHIASM dobry\n",
    "# ABIDE dobry\n",
    "# Athletes dobry\n",
    "# COBRE dobry\n",
    "# HCP dobry\n",
    "# Leipzig good\n",
    "# MCIC złe\n",
    "# UoN dobry\n",
    "\n",
    "# Splits\n",
    "train_split = 0.85\n",
    "dev_split = 0.15\n",
    "test_split = 0.0\n",
    "\n",
    "# Dictionary with study design\n",
    "design = {}\n",
    "\n",
    "design['train']={}\n",
    "design['dev_train']={}\n",
    "design['dev_test']={}\n",
    "design['test']={}\n",
    "\n",
    "design['train']['all']={}\n",
    "design['dev_train']['all']={}\n",
    "design['dev_test']['all']={}\n",
    "design['test']['all']={}\n",
    "\n",
    "# Training data\n",
    "for group in train_groups:\n",
    "    \n",
    "    #design['dev_train'][group]={}\n",
    "    #design['test'][group]={}\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    # Randomize order\n",
    "    random.shuffle(ids) \n",
    "    # Find split ratios\n",
    "    train_idx = np.int(np.floor(len(ids)*train_split))\n",
    "    dev_idx = np.int(np.floor(len(ids)*(train_split+dev_split)))\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+ids[i]+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "    \n",
    "        if i+1<=train_idx:\n",
    "            design['train']['all'][ids[i]]=files\n",
    "        elif i+1 > dev_idx:\n",
    "            design['test']['all'][ids[i]]=files\n",
    "            #design['test'][group][ids[i]]=files\n",
    "        else:\n",
    "            design['dev_train']['all'][ids[i]]=files\n",
    "            #design['dev_train'][group][ids[i]]=files\n",
    "        \n",
    "# Dev data\n",
    "for group in test_dev_groups:\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    \n",
    "    for sub_id in ids:\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "        \n",
    "        design['dev_test']['all'][sub_id]=files\n",
    "    \n",
    "# Test data\n",
    "for group in test_groups:\n",
    "    \n",
    "    #design['test'][group]={}\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    \n",
    "    for sub_id in ids:\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "        \n",
    "        design['test']['all'][sub_id]=files \n",
    "        #design['test'][group][sub_id]=files  \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary\n",
    "#with open('study_design.pkl', 'wb') as f:\n",
    "#    pickle.dump(design, f)\n",
    "\n",
    "# Load the dictionary\n",
    "with open('study_design.pkl', 'rb') as f:\n",
    "    design = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all images\n",
    "subjects_list={}\n",
    "\n",
    "for group in design.keys():\n",
    "    subjects_list[group]={}\n",
    "    \n",
    "    for dataset in design[group].keys():\n",
    "        subjects_list[group][dataset]= [tio.Subject(t1=tio.Image(design[group][dataset][sub]['brain'], type=tio.INTENSITY),\n",
    "                            probs = tio.Image(design[group][dataset][sub]['probs'], type = tio.INTENSITY)) for sub in design[group][dataset].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale\n",
    "rescale = RescaleIntensity((0,1))\n",
    "# Flip\n",
    "flip = RandomFlip((0,1,2), flip_probability=0.5, p=0.25)\n",
    "# Affine transformations\n",
    "#affine = RandomAffine(degrees=30)\n",
    "\n",
    "# Composing transforms - rescaling is mandatory, training data is subjected to a range of additional augmentations\n",
    "transform_train = Compose([rescale, flip]) # leaving out standardization for now\n",
    "transform_dev = Compose([rescale]) # leaving out standardization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchio's (Pytorch's) Dataset\n",
    "data = {}\n",
    "\n",
    "for group in subjects_list.keys():\n",
    "    data[group]={}\n",
    "    \n",
    "    for dataset in subjects_list[group]:\n",
    "        \n",
    "        if group == 'train':\n",
    "            data[group][dataset]=tio.SubjectsDataset(subjects_list[group][dataset], transform=transform_train)\n",
    "        else:\n",
    "            data[group][dataset]=tio.SubjectsDataset(subjects_list[group][dataset], transform=transform_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler\n",
    "patch_size = (24,24,8)\n",
    "queue_length = 500\n",
    "samples_per_volume = 5\n",
    "\n",
    "sampler = tio.data.WeightedSampler(patch_size,'probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "dataloader = {}\n",
    "\n",
    "for group in data.keys():\n",
    "    dataloader[group]={}\n",
    "    \n",
    "    for dataset in data[group]:\n",
    "        \n",
    "        dataloader[group][dataset]=DataLoader(tio.Queue(data[group][dataset], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size = 25, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_epochs = 1\\n\\nmodel = torch.nn.Identity()\\n\\nfor epoch_index in range(num_epochs):\\n    for patches_batch in dataloader['dev_train']['all']:\\n        #print(patches_batch)\\n        inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\\n        targets = patches_batch['t1'][tio.DATA]  # key 'brain' is in subject\\n        logits = model(inputs)  # model being an instance of torch.nn.Module\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "'''\n",
    "num_epochs = 1\n",
    "\n",
    "model = torch.nn.Identity()\n",
    "\n",
    "for epoch_index in range(num_epochs):\n",
    "    for patches_batch in dataloader['dev_train']['all']:\n",
    "        #print(patches_batch)\n",
    "        inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\n",
    "        targets = patches_batch['t1'][tio.DATA]  # key 'brain' is in subject\n",
    "        logits = model(inputs)  # model being an instance of torch.nn.Module\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfig = plt.figure(figsize=(20, 10))\\n\\nfor i in range(inputs.shape[0]):\\n    plt.subplot(5,8,i+1)\\n    plt.imshow(inputs[i,0,:,:,5],cmap='gray');\\n    \\nplt.show()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs.shape\n",
    "'''\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(inputs.shape[0]):\n",
    "    plt.subplot(5,8,i+1)\n",
    "    plt.imshow(inputs[i,0,:,:,5],cmap='gray');\n",
    "    \n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torchio as tio\\nt1 = tio.ScalarImage('T1w')\\nt2 = tio.ScalarImage('T2w')\\nsubject = tio.Subject(T1w=t1, T2w=t2)\\ncp = tio.CropOrPad((512, 512, 408))\\nsubject = tio.Subject(T1w=cp(t1), T2w=cp(t2))\\nsubject.plot(reorient=False)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torchio as tio\n",
    "t1 = tio.ScalarImage('T1w')\n",
    "t2 = tio.ScalarImage('T2w')\n",
    "subject = tio.Subject(T1w=t1, T2w=t2)\n",
    "cp = tio.CropOrPad((512, 512, 408))\n",
    "subject = tio.Subject(T1w=cp(t1), T2w=cp(t2))\n",
    "subject.plot(reorient=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Try setting CUDA if possible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") \n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropped U-Net copied from Overfitting Model\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=10, scaling=2):\n",
    "        super(UNet, self).__init__()\n",
    "                \n",
    "        # Encoding layers\n",
    "        self.encoder1 = self.unet_block(in_channels, init_features, \"enc1\")\n",
    "        self.pool1 = nn.AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.encoder2 = self.unet_block(init_features, init_features*scaling, name='enc2')\n",
    "        self.pool2 = nn.AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        self.bottleneck = self.unet_block(init_features*scaling, init_features*scaling**2, name='bottleneck')\n",
    "        \n",
    "        # Decoding layers (where merge with prevois encoding layers occurs)        \n",
    "        self.upconv2 = nn.ConvTranspose3d(init_features*scaling**2, init_features*scaling, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.unet_block(init_features*scaling, init_features*scaling, name='dec2')\n",
    "                \n",
    "        self.upconv1 = nn.ConvTranspose3d(init_features*scaling, init_features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.unet_block(init_features, init_features, name='dec1')\n",
    "        \n",
    "        # Final convolution - output equals number of output channels\n",
    "        self.conv = nn.Conv3d(init_features, out_channels, kernel_size=1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        # Upconvolving, concatenating data from respective encoding phase and executing UNet block\n",
    "        dec2 = self.upconv2(bottleneck)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        out_conv = self.conv(dec1)\n",
    "        \n",
    "        return torch.sigmoid(out_conv)\n",
    "    \n",
    "    def unet_block(self, in_channels, features, name):\n",
    "        \n",
    "        return nn.Sequential(OrderedDict([(name+'conv1',nn.Conv3d(in_channels=in_channels, out_channels=features, kernel_size=3, padding=1, bias=False)),\n",
    "                             (name+'bnorm1', nn.BatchNorm3d(num_features=features)),\n",
    "                             (name+'relu1', nn.ReLU(inplace=True)),\n",
    "                             (name+'conv2', nn.Conv3d(in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False)),\n",
    "                             (name+'bnorm2', nn.BatchNorm3d(num_features=features)),\n",
    "                             (name+'relu2', nn.ReLU(inplace=True))])\n",
    "                            )\n",
    "\n",
    "    def output_latent_representations(self,x):\n",
    "        \n",
    "        print(x.shape)\n",
    "\n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "        \n",
    "        print(bottleneck.shape)\n",
    "        \n",
    "        return bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet = UNet(1,1,2,2) # Size of latent representation = (1/64) * init_features * scaling**2\n",
    "#unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sum(p.numel() for p in unet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noutputs = unet(inputs.to(device))\\noutputs = outputs.cpu().detach().numpy()\\n\\nfig = plt.figure(figsize=(20, 10))\\n\\nfor i in range(outputs.shape[0]):\\n    plt.subplot(10,10,i+1)\\n    plt.imshow(outputs[i,0,:,:,5],cmap='gray');\\n    \\nplt.show()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try processing\n",
    "'''\n",
    "outputs = unet(inputs.to(device))\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(outputs.shape[0]):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(outputs[i,0,:,:,5],cmap='gray');\n",
    "    \n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "#criterion = DiceLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcome = criterion(inputs, torch.Tensor(outputs))\n",
    "#print(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning trained model\n",
    "def train_network(n_epochs, dataloaders, model, optimizer, criterion, device, save_path):\n",
    "    \n",
    "    track_train_loss = []\n",
    "    track_dev_train_loss = []\n",
    "    track_dev_test_loss = []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        # Initialize loss monitoring variables\n",
    "        train_loss = 0.0\n",
    "        dev_train_loss = 0.0\n",
    "        dev_test_loss = 0.0\n",
    "        \n",
    "        i=0\n",
    "        k=0\n",
    "        j=0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        \n",
    "        for batch in dataloaders['train']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            i+=1\n",
    "            \n",
    "        track_train_loss.append(train_loss/i)\n",
    "        \n",
    "        # Validation on two datasets\n",
    "        model.eval()\n",
    "        \n",
    "        for batch in dataloaders['dev_train']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                dev_train_loss += loss.item()\n",
    "                j+=1\n",
    "                \n",
    "        track_dev_train_loss.append(dev_train_loss/j)\n",
    "        \n",
    "        \n",
    "        for batch in dataloaders['dev_test']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                dev_test_loss += loss.item()\n",
    "                k+=1\n",
    "                \n",
    "        track_dev_test_loss.append(dev_test_loss/k)\n",
    "        \n",
    "        # Print summary of epoch\n",
    "        duration = time.time() - start\n",
    "\n",
    "        print('END OF EPOCH: {} \\tTraining loss per batch: {:.6f}\\tTraining_dev loss per batch: {:.6f}\\tTest_dev loss per batch: {:.6f}'.format(epoch, train_loss/i, dev_train_loss/j, dev_test_loss/k))\n",
    "       \n",
    "        \n",
    "        ## Save the model if reached min validation loss\n",
    "        if dev_train_loss + dev_test_loss < valid_loss_min:\n",
    "            valid_loss_min = dev_train_loss + dev_test_loss\n",
    "            torch.save(model.state_dict(),save_path+'optimal_weights')\n",
    "                        \n",
    "    # return trained model\n",
    "    return track_train_loss, track_dev_train_loss, track_dev_test_loss         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [30:41<12:16:35, 1841.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.019672\tTraining_dev loss per batch: 0.006682\tTest_dev loss per batch: 0.005112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/25 [58:35<11:26:37, 1791.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.006365\tTraining_dev loss per batch: 0.004665\tTest_dev loss per batch: 0.004465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:26:43<10:45:26, 1760.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005326\tTraining_dev loss per batch: 0.004054\tTest_dev loss per batch: 0.003704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:53:50<10:02:05, 1720.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004953\tTraining_dev loss per batch: 0.004594\tTest_dev loss per batch: 0.004208\n",
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004605\tTraining_dev loss per batch: 0.003638\tTest_dev loss per batch: 0.003112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [2:49:32<8:57:49, 1698.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004396\tTraining_dev loss per batch: 0.003294\tTest_dev loss per batch: 0.002827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [3:18:34<8:33:24, 1711.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003750\tTraining_dev loss per batch: 0.002728\tTest_dev loss per batch: 0.001148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [3:49:22<8:16:30, 1752.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003704\tTraining_dev loss per batch: 0.002681\tTest_dev loss per batch: 0.000914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [4:19:34<7:52:04, 1770.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003493\tTraining_dev loss per batch: 0.002556\tTest_dev loss per batch: 0.000686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 10/25 [4:48:19<7:19:08, 1756.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003411\tTraining_dev loss per batch: 0.002545\tTest_dev loss per batch: 0.000801\n",
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003107\tTraining_dev loss per batch: 0.002235\tTest_dev loss per batch: 0.000648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [5:47:32<6:22:14, 1764.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003327\tTraining_dev loss per batch: 0.004836\tTest_dev loss per batch: 0.000831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [6:16:11<5:50:06, 1750.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003021\tTraining_dev loss per batch: 0.002318\tTest_dev loss per batch: 0.000655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:43:33<5:14:58, 1718.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003089\tTraining_dev loss per batch: 0.002257\tTest_dev loss per batch: 0.000541\n",
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003017\tTraining_dev loss per batch: 0.002173\tTest_dev loss per batch: 0.000700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [7:40:16<4:16:52, 1712.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003068\tTraining_dev loss per batch: 0.002973\tTest_dev loss per batch: 0.000573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [8:09:08<3:49:06, 1718.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003133\tTraining_dev loss per batch: 0.002522\tTest_dev loss per batch: 0.000635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [8:37:28<3:19:48, 1712.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003053\tTraining_dev loss per batch: 0.002678\tTest_dev loss per batch: 0.000736\n",
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002974\tTraining_dev loss per batch: 0.001954\tTest_dev loss per batch: 0.000563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [9:31:36<2:18:54, 1666.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.002795\tTraining_dev loss per batch: 0.002270\tTest_dev loss per batch: 0.000595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 21/25 [9:58:50<1:50:27, 1656.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002908\tTraining_dev loss per batch: 0.001997\tTest_dev loss per batch: 0.000514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [10:26:50<1:23:11, 1663.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.003055\tTraining_dev loss per batch: 0.002640\tTest_dev loss per batch: 0.001298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [10:54:46<55:34, 1667.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.002984\tTraining_dev loss per batch: 0.002192\tTest_dev loss per batch: 0.000521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [11:22:06<27:39, 1659.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002873\tTraining_dev loss per batch: 0.002336\tTest_dev loss per batch: 0.000742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:48:13<00:00, 1699.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.002953\tTraining_dev loss per batch: 0.002306\tTest_dev loss per batch: 0.000649\n",
      "[16, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.008059\tTraining_dev loss per batch: 0.003544\tTest_dev loss per batch: 0.000874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [55:58<10:43:38, 1679.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004726\tTraining_dev loss per batch: 0.005412\tTest_dev loss per batch: 0.001281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:22:46<10:07:50, 1657.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.004387\tTraining_dev loss per batch: 0.004115\tTest_dev loss per batch: 0.001328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:50:16<9:39:24, 1655.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004119\tTraining_dev loss per batch: 0.004143\tTest_dev loss per batch: 0.001449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 5/25 [2:17:19<9:08:37, 1645.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004088\tTraining_dev loss per batch: 0.003597\tTest_dev loss per batch: 0.002288\n",
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003572\tTraining_dev loss per batch: 0.003157\tTest_dev loss per batch: 0.001082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [3:12:38<8:14:56, 1649.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003370\tTraining_dev loss per batch: 0.003049\tTest_dev loss per batch: 0.001025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [3:40:21<7:48:34, 1653.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003283\tTraining_dev loss per batch: 0.002556\tTest_dev loss per batch: 0.000751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [4:07:43<7:19:59, 1650.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003326\tTraining_dev loss per batch: 0.002645\tTest_dev loss per batch: 0.000621\n",
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003077\tTraining_dev loss per batch: 0.002499\tTest_dev loss per batch: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [5:01:10<6:20:43, 1631.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003124\tTraining_dev loss per batch: 0.002471\tTest_dev loss per batch: 0.001148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [5:29:29<5:57:54, 1651.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003105\tTraining_dev loss per batch: 0.002147\tTest_dev loss per batch: 0.000568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [5:56:34<5:28:44, 1643.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003213\tTraining_dev loss per batch: 0.003187\tTest_dev loss per batch: 0.000709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:24:03<5:01:39, 1645.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003090\tTraining_dev loss per batch: 0.002252\tTest_dev loss per batch: 0.000753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [6:51:01<4:32:52, 1637.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003067\tTraining_dev loss per batch: 0.002564\tTest_dev loss per batch: 0.000496\n",
      "END OF EPOCH: 16 \tTraining loss per batch: 0.002766\tTraining_dev loss per batch: 0.002052\tTest_dev loss per batch: 0.001059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [7:45:24<3:37:43, 1632.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.002832\tTraining_dev loss per batch: 0.002021\tTest_dev loss per batch: 0.000597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [8:13:55<3:13:13, 1656.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.002798\tTraining_dev loss per batch: 0.002130\tTest_dev loss per batch: 0.001192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:41:31<2:45:38, 1656.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002809\tTraining_dev loss per batch: 0.009669\tTest_dev loss per batch: 0.001676\n",
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003002\tTraining_dev loss per batch: 0.001899\tTest_dev loss per batch: 0.000571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [9:38:17<1:52:49, 1692.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002752\tTraining_dev loss per batch: 0.002042\tTest_dev loss per batch: 0.000541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [10:06:33<1:24:39, 1693.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.002775\tTraining_dev loss per batch: 0.002935\tTest_dev loss per batch: 0.000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [10:33:53<55:54, 1677.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.002648\tTraining_dev loss per batch: 0.002128\tTest_dev loss per batch: 0.000869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [11:00:21<27:30, 1650.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002607\tTraining_dev loss per batch: 0.002079\tTest_dev loss per batch: 0.000474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:27:00<00:00, 1648.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.002660\tTraining_dev loss per batch: 0.002553\tTest_dev loss per batch: 0.000984\n",
      "[4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.008943\tTraining_dev loss per batch: 0.003179\tTest_dev loss per batch: 0.001378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [55:29<10:39:10, 1667.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004782\tTraining_dev loss per batch: 0.003319\tTest_dev loss per batch: 0.001100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:23:16<10:11:18, 1667.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.004393\tTraining_dev loss per batch: 0.003549\tTest_dev loss per batch: 0.001809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:50:39<9:41:01, 1660.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004059\tTraining_dev loss per batch: 0.003489\tTest_dev loss per batch: 0.000802\n",
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004009\tTraining_dev loss per batch: 0.002459\tTest_dev loss per batch: 0.000799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [2:44:39<8:39:05, 1639.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003883\tTraining_dev loss per batch: 0.003195\tTest_dev loss per batch: 0.000906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [3:12:59<8:17:13, 1657.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003829\tTraining_dev loss per batch: 0.002811\tTest_dev loss per batch: 0.000686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [3:40:31<7:49:05, 1655.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003755\tTraining_dev loss per batch: 0.002483\tTest_dev loss per batch: 0.001113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [4:07:03<7:16:27, 1636.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003737\tTraining_dev loss per batch: 0.002864\tTest_dev loss per batch: 0.000581\n",
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003613\tTraining_dev loss per batch: 0.002441\tTest_dev loss per batch: 0.000591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [5:01:15<6:21:00, 1632.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003365\tTraining_dev loss per batch: 0.002269\tTest_dev loss per batch: 0.000748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [5:29:13<5:56:42, 1646.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003233\tTraining_dev loss per batch: 0.002689\tTest_dev loss per batch: 0.000679\n",
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003431\tTraining_dev loss per batch: 0.002261\tTest_dev loss per batch: 0.000737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [6:22:51<4:58:43, 1629.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003340\tTraining_dev loss per batch: 0.002701\tTest_dev loss per batch: 0.000744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [6:49:47<4:30:53, 1625.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003233\tTraining_dev loss per batch: 0.002441\tTest_dev loss per batch: 0.001151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 16/25 [7:16:19<4:02:16, 1615.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003082\tTraining_dev loss per batch: 0.037388\tTest_dev loss per batch: 0.003384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [7:44:03<3:37:17, 1629.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003202\tTraining_dev loss per batch: 0.006544\tTest_dev loss per batch: 0.000989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [8:11:21<3:10:25, 1632.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003239\tTraining_dev loss per batch: 0.003244\tTest_dev loss per batch: 0.001095\n",
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002919\tTraining_dev loss per batch: 0.002177\tTest_dev loss per batch: 0.000662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [9:05:10<2:15:21, 1624.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.002870\tTraining_dev loss per batch: 0.002231\tTest_dev loss per batch: 0.000948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 21/25 [9:32:58<1:49:09, 1637.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002935\tTraining_dev loss per batch: 0.003432\tTest_dev loss per batch: 0.001134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [10:00:47<1:22:20, 1646.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.002881\tTraining_dev loss per batch: 0.002488\tTest_dev loss per batch: 0.000674\n",
      "END OF EPOCH: 23 \tTraining loss per batch: 0.002669\tTraining_dev loss per batch: 0.002189\tTest_dev loss per batch: 0.000528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [10:54:56<27:17, 1637.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002710\tTraining_dev loss per batch: 0.002447\tTest_dev loss per batch: 0.000753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:20:57<00:00, 1634.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.003057\tTraining_dev loss per batch: 0.002239\tTest_dev loss per batch: 0.001987\n",
      "[2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.012598\tTraining_dev loss per batch: 0.005848\tTest_dev loss per batch: 0.001679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [54:23<10:19:38, 1616.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.005998\tTraining_dev loss per batch: 0.005477\tTest_dev loss per batch: 0.001433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:21:37<9:54:39, 1621.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005635\tTraining_dev loss per batch: 0.004690\tTest_dev loss per batch: 0.001214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:48:49<9:28:42, 1624.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.005004\tTraining_dev loss per batch: 0.005135\tTest_dev loss per batch: 0.001375\n",
      "END OF EPOCH: 5 \tTraining loss per batch: 0.005056\tTraining_dev loss per batch: 0.004290\tTest_dev loss per batch: 0.001144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [2:42:00<8:31:35, 1615.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004594\tTraining_dev loss per batch: 0.004446\tTest_dev loss per batch: 0.002561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [3:09:54<8:09:55, 1633.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004581\tTraining_dev loss per batch: 0.004129\tTest_dev loss per batch: 0.002216\n",
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004821\tTraining_dev loss per batch: 0.003943\tTest_dev loss per batch: 0.001361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [4:03:38<7:12:36, 1622.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.004278\tTraining_dev loss per batch: 0.004418\tTest_dev loss per batch: 0.000932\n",
      "END OF EPOCH: 10 \tTraining loss per batch: 0.004287\tTraining_dev loss per batch: 0.003446\tTest_dev loss per batch: 0.000894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [4:57:14<6:17:13, 1616.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.004241\tTraining_dev loss per batch: 0.003293\tTest_dev loss per batch: 0.000891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [5:24:13<5:50:24, 1617.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.004199\tTraining_dev loss per batch: 0.003628\tTest_dev loss per batch: 0.000893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [5:51:54<5:26:02, 1630.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.004262\tTraining_dev loss per batch: 0.003341\tTest_dev loss per batch: 0.001389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:19:14<4:59:27, 1633.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.004267\tTraining_dev loss per batch: 0.003842\tTest_dev loss per batch: 0.002724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [6:45:18<4:28:44, 1612.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.004260\tTraining_dev loss per batch: 0.003371\tTest_dev loss per batch: 0.000798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 16/25 [7:12:44<4:03:24, 1622.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.004189\tTraining_dev loss per batch: 0.003554\tTest_dev loss per batch: 0.001142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [7:40:40<3:38:29, 1638.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003908\tTraining_dev loss per batch: 0.003600\tTest_dev loss per batch: 0.001283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [8:08:26<3:12:07, 1646.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.004000\tTraining_dev loss per batch: 0.003366\tTest_dev loss per batch: 0.000834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:34:34<2:42:18, 1623.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.004043\tTraining_dev loss per batch: 0.003302\tTest_dev loss per batch: 0.000995\n",
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003978\tTraining_dev loss per batch: 0.003269\tTest_dev loss per batch: 0.000946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [9:28:24<1:47:52, 1618.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.004125\tTraining_dev loss per batch: 0.002980\tTest_dev loss per batch: 0.000833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [9:55:30<1:21:01, 1620.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.003997\tTraining_dev loss per batch: 0.003324\tTest_dev loss per batch: 0.001058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [10:23:05<54:21, 1630.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.003862\tTraining_dev loss per batch: 0.003488\tTest_dev loss per batch: 0.000835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [10:50:26<27:13, 1633.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.003935\tTraining_dev loss per batch: 0.003242\tTest_dev loss per batch: 0.001176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:16:22<00:00, 1623.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.003803\tTraining_dev loss per batch: 0.003428\tTest_dev loss per batch: 0.001050\n",
      "[8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.012381\tTraining_dev loss per batch: 0.011879\tTest_dev loss per batch: 0.004082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [55:34<10:39:46, 1668.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004809\tTraining_dev loss per batch: 0.003854\tTest_dev loss per batch: 0.002754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:23:31<10:12:51, 1671.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.004563\tTraining_dev loss per batch: 0.005315\tTest_dev loss per batch: 0.003669\n",
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004351\tTraining_dev loss per batch: 0.002890\tTest_dev loss per batch: 0.001024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [2:17:04<9:06:16, 1638.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.003692\tTraining_dev loss per batch: 0.002624\tTest_dev loss per batch: 0.000778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 6/25 [2:44:18<8:38:25, 1637.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003910\tTraining_dev loss per batch: 0.002935\tTest_dev loss per batch: 0.001780\n",
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003483\tTraining_dev loss per batch: 0.002517\tTest_dev loss per batch: 0.000795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [3:39:13<7:46:10, 1645.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003495\tTraining_dev loss per batch: 0.007098\tTest_dev loss per batch: 0.000888\n",
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003222\tTraining_dev loss per batch: 0.002248\tTest_dev loss per batch: 0.001433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [4:33:06<6:46:57, 1627.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003204\tTraining_dev loss per batch: 0.002310\tTest_dev loss per batch: 0.000721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 11/25 [4:59:48<6:18:00, 1620.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003356\tTraining_dev loss per batch: 0.002006\tTest_dev loss per batch: 0.000612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [5:27:49<5:54:57, 1638.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003017\tTraining_dev loss per batch: 0.002029\tTest_dev loss per batch: 0.000651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [5:55:15<5:28:10, 1640.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.002971\tTraining_dev loss per batch: 0.002004\tTest_dev loss per batch: 0.000691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:21:44<4:57:56, 1625.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.002792\tTraining_dev loss per batch: 0.002032\tTest_dev loss per batch: 0.000498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [6:48:02<4:28:29, 1610.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.002839\tTraining_dev loss per batch: 0.002366\tTest_dev loss per batch: 0.000791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 16/25 [7:15:15<4:02:39, 1617.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.002799\tTraining_dev loss per batch: 0.002155\tTest_dev loss per batch: 0.000705\n",
      "END OF EPOCH: 17 \tTraining loss per batch: 0.002970\tTraining_dev loss per batch: 0.001929\tTest_dev loss per batch: 0.000682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [8:08:54<3:07:56, 1610.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.002782\tTraining_dev loss per batch: 0.001961\tTest_dev loss per batch: 0.000677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:35:49<2:41:13, 1612.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002693\tTraining_dev loss per batch: 0.002032\tTest_dev loss per batch: 0.000739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 20/25 [9:02:26<2:13:57, 1607.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.002599\tTraining_dev loss per batch: 0.002282\tTest_dev loss per batch: 0.001571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 21/25 [9:28:40<1:46:30, 1597.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002614\tTraining_dev loss per batch: 0.002616\tTest_dev loss per batch: 0.000650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [9:56:17<1:20:45, 1615.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.002582\tTraining_dev loss per batch: 0.002055\tTest_dev loss per batch: 0.000789\n",
      "END OF EPOCH: 23 \tTraining loss per batch: 0.002718\tTraining_dev loss per batch: 0.001854\tTest_dev loss per batch: 0.000517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [10:50:47<27:04, 1624.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002472\tTraining_dev loss per batch: 0.001937\tTest_dev loss per batch: 0.000464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:16:21<00:00, 1623.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.002359\tTraining_dev loss per batch: 0.001971\tTest_dev loss per batch: 0.000517\n",
      "[4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.011719\tTraining_dev loss per batch: 0.007038\tTest_dev loss per batch: 0.002511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [54:28<10:24:02, 1627.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004782\tTraining_dev loss per batch: 0.003373\tTest_dev loss per batch: 0.001325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:20:41<9:50:56, 1611.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.004172\tTraining_dev loss per batch: 0.005757\tTest_dev loss per batch: 0.005255\n",
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004288\tTraining_dev loss per batch: 0.003055\tTest_dev loss per batch: 0.000900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [2:13:03<8:49:34, 1588.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.003921\tTraining_dev loss per batch: 0.004553\tTest_dev loss per batch: 0.004050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 6/25 [2:40:15<8:27:14, 1601.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003961\tTraining_dev loss per batch: 0.002647\tTest_dev loss per batch: 0.000927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [3:07:07<8:01:28, 1604.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003607\tTraining_dev loss per batch: 0.005697\tTest_dev loss per batch: 0.007376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [3:34:13<7:36:30, 1611.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003631\tTraining_dev loss per batch: 0.003170\tTest_dev loss per batch: 0.000635\n",
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003530\tTraining_dev loss per batch: 0.002463\tTest_dev loss per batch: 0.001132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [4:26:48<6:37:15, 1589.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003382\tTraining_dev loss per batch: 0.003580\tTest_dev loss per batch: 0.001500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 11/25 [4:54:06<6:14:14, 1603.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003377\tTraining_dev loss per batch: 0.002550\tTest_dev loss per batch: 0.001122\n",
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003311\tTraining_dev loss per batch: 0.002294\tTest_dev loss per batch: 0.000650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [5:47:18<5:19:10, 1595.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003248\tTraining_dev loss per batch: 0.003234\tTest_dev loss per batch: 0.001556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:13:39<4:51:47, 1591.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003118\tTraining_dev loss per batch: 0.002519\tTest_dev loss per batch: 0.001069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [6:39:30<4:23:14, 1579.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.002906\tTraining_dev loss per batch: 0.002995\tTest_dev loss per batch: 0.001288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 16/25 [7:06:17<3:58:08, 1587.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003074\tTraining_dev loss per batch: 0.002591\tTest_dev loss per batch: 0.000726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [7:33:02<3:32:22, 1592.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003047\tTraining_dev loss per batch: 0.002765\tTest_dev loss per batch: 0.000653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [7:59:31<3:05:42, 1591.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003189\tTraining_dev loss per batch: 0.002331\tTest_dev loss per batch: 0.001037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:26:19<2:39:39, 1596.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002998\tTraining_dev loss per batch: 0.003330\tTest_dev loss per batch: 0.000830\n",
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003001\tTraining_dev loss per batch: 0.001999\tTest_dev loss per batch: 0.000593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [9:18:41<1:45:53, 1588.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002710\tTraining_dev loss per batch: 0.002138\tTest_dev loss per batch: 0.000620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [9:45:51<1:20:01, 1600.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.002696\tTraining_dev loss per batch: 0.002184\tTest_dev loss per batch: 0.000706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [10:13:05<53:41, 1610.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.003048\tTraining_dev loss per batch: 0.002058\tTest_dev loss per batch: 0.001006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [10:39:20<26:39, 1599.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002899\tTraining_dev loss per batch: 0.002053\tTest_dev loss per batch: 0.001172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:05:51<00:00, 1598.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.002780\tTraining_dev loss per batch: 0.003271\tTest_dev loss per batch: 0.002742\n",
      "[64, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.010058\tTraining_dev loss per batch: 0.004914\tTest_dev loss per batch: 0.002205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [53:11<10:13:37, 1600.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.005674\tTraining_dev loss per batch: 0.004042\tTest_dev loss per batch: 0.001229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:21:00<9:54:31, 1621.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005421\tTraining_dev loss per batch: 0.003608\tTest_dev loss per batch: 0.001113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:47:39<9:25:08, 1614.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004797\tTraining_dev loss per batch: 0.019695\tTest_dev loss per batch: 0.001060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 5/25 [2:13:56<8:54:24, 1603.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004972\tTraining_dev loss per batch: 0.004396\tTest_dev loss per batch: 0.001639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 6/25 [2:39:54<8:23:26, 1589.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004843\tTraining_dev loss per batch: 0.003705\tTest_dev loss per batch: 0.001391\n",
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004171\tTraining_dev loss per batch: 0.003220\tTest_dev loss per batch: 0.000885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [3:33:28<7:32:33, 1597.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004025\tTraining_dev loss per batch: 0.002610\tTest_dev loss per batch: 0.000622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [3:59:19<7:02:11, 1583.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003488\tTraining_dev loss per batch: 0.003050\tTest_dev loss per batch: 0.002329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 10/25 [4:25:23<6:34:24, 1577.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003508\tTraining_dev loss per batch: 0.003160\tTest_dev loss per batch: 0.000893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 11/25 [4:51:56<6:09:11, 1582.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003504\tTraining_dev loss per batch: 0.002757\tTest_dev loss per batch: 0.001310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [5:19:25<5:47:08, 1602.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003304\tTraining_dev loss per batch: 0.003862\tTest_dev loss per batch: 0.000786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [5:45:49<5:19:21, 1596.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003194\tTraining_dev loss per batch: 0.004782\tTest_dev loss per batch: 0.001083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:12:30<4:52:58, 1598.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003293\tTraining_dev loss per batch: 0.002768\tTest_dev loss per batch: 0.000764\n",
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003050\tTraining_dev loss per batch: 0.002514\tTest_dev loss per batch: 0.000720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [7:05:29<3:58:55, 1592.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003154\tTraining_dev loss per batch: 0.003046\tTest_dev loss per batch: 0.001884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [7:32:08<3:32:38, 1594.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003066\tTraining_dev loss per batch: 0.002078\tTest_dev loss per batch: 0.000943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [7:58:55<3:06:27, 1598.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.002768\tTraining_dev loss per batch: 0.002578\tTest_dev loss per batch: 0.000519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:24:49<2:38:29, 1584.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.003017\tTraining_dev loss per batch: 0.002273\tTest_dev loss per batch: 0.000982\n",
      "END OF EPOCH: 20 \tTraining loss per batch: 0.002828\tTraining_dev loss per batch: 0.002065\tTest_dev loss per batch: 0.000613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [9:19:27<1:47:53, 1618.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002755\tTraining_dev loss per batch: 0.002133\tTest_dev loss per batch: 0.000505\n",
      "END OF EPOCH: 22 \tTraining loss per batch: 0.002505\tTraining_dev loss per batch: 0.001926\tTest_dev loss per batch: 0.000433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [10:13:20<53:43, 1611.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.002418\tTraining_dev loss per batch: 0.001931\tTest_dev loss per batch: 0.000591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [10:39:47<26:44, 1604.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.002410\tTraining_dev loss per batch: 0.002199\tTest_dev loss per batch: 0.000457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:05:15<00:00, 1596.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.002075\tTraining_dev loss per batch: 0.001933\tTest_dev loss per batch: 0.000414\n",
      "[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.041220\tTraining_dev loss per batch: 0.025990\tTest_dev loss per batch: 0.045805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [53:53<10:14:10, 1602.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.024356\tTraining_dev loss per batch: 0.024706\tTest_dev loss per batch: 0.036040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [1:20:00<9:43:36, 1591.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.024396\tTraining_dev loss per batch: 0.024767\tTest_dev loss per batch: 0.033499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [1:46:16<9:15:28, 1587.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.014964\tTraining_dev loss per batch: 0.008471\tTest_dev loss per batch: 0.013197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 5/25 [2:11:33<8:41:59, 1565.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.007091\tTraining_dev loss per batch: 0.005160\tTest_dev loss per batch: 0.008455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 6/25 [2:37:56<8:17:31, 1571.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.005839\tTraining_dev loss per batch: 0.004640\tTest_dev loss per batch: 0.005738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [3:04:43<7:54:32, 1581.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.005206\tTraining_dev loss per batch: 0.003921\tTest_dev loss per batch: 0.004438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [3:30:24<7:24:40, 1569.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004928\tTraining_dev loss per batch: 0.005084\tTest_dev loss per batch: 0.004136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [3:56:51<6:59:58, 1574.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.004749\tTraining_dev loss per batch: 0.004939\tTest_dev loss per batch: 0.005642\n",
      "END OF EPOCH: 10 \tTraining loss per batch: 0.004553\tTraining_dev loss per batch: 0.003626\tTest_dev loss per batch: 0.003295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [4:48:54<6:06:38, 1571.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.004575\tTraining_dev loss per batch: 0.007114\tTest_dev loss per batch: 0.003570\n",
      "END OF EPOCH: 12 \tTraining loss per batch: 0.004256\tTraining_dev loss per batch: 0.003336\tTest_dev loss per batch: 0.002595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [5:42:03<5:16:49, 1584.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.004179\tTraining_dev loss per batch: 0.002986\tTest_dev loss per batch: 0.002380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [6:08:23<4:50:11, 1582.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.004205\tTraining_dev loss per batch: 0.003646\tTest_dev loss per batch: 0.002374\n",
      "END OF EPOCH: 15 \tTraining loss per batch: 0.004180\tTraining_dev loss per batch: 0.002888\tTest_dev loss per batch: 0.001980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [7:00:14<3:56:00, 1573.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.004053\tTraining_dev loss per batch: 0.003918\tTest_dev loss per batch: 0.002251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [7:26:38<3:30:13, 1576.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.004188\tTraining_dev loss per batch: 0.003125\tTest_dev loss per batch: 0.001632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [7:52:53<3:03:51, 1575.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.004056\tTraining_dev loss per batch: 0.004213\tTest_dev loss per batch: 0.002383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [8:17:59<2:35:29, 1554.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.003993\tTraining_dev loss per batch: 0.003169\tTest_dev loss per batch: 0.001846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 20/25 [8:43:11<2:08:30, 1542.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003947\tTraining_dev loss per batch: 0.004722\tTest_dev loss per batch: 0.002591\n",
      "END OF EPOCH: 21 \tTraining loss per batch: 0.003892\tTraining_dev loss per batch: 0.002847\tTest_dev loss per batch: 0.001692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [9:35:36<1:17:50, 1556.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.003588\tTraining_dev loss per batch: 0.002910\tTest_dev loss per batch: 0.001458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [10:02:18<52:20, 1570.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.003874\tTraining_dev loss per batch: 0.003343\tTest_dev loss per batch: 0.002313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [10:29:07<26:21, 1581.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.003660\tTraining_dev loss per batch: 0.004716\tTest_dev loss per batch: 0.002492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [10:54:13<00:00, 1570.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.003753\tTraining_dev loss per batch: 0.003596\tTest_dev loss per batch: 0.002003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#model_parameters=[[64,1],[32,1],[16,2],[16,1],[8,2],[8,1],[4,4],[4,2],[4,1],[2,4],[2,2],[2,1],[1,8],[1,4],[1,2],[1,1]]\n",
    "model_parameters=[[2,4],[16,1],[4,2],[2,2],[8,2],[4,4],[64,1],[1,8]]\n",
    "\n",
    "folder='../../1_Data/2_Trained_AE/'\n",
    "\n",
    "for parameters in model_parameters:\n",
    "    \n",
    "    print(parameters)\n",
    "        \n",
    "    # Initialize the proper model\n",
    "    unet = UNet(1,1,parameters[0],parameters[1])\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(params=unet.parameters(), lr=0.005)\n",
    "    \n",
    "    # Create output folder\n",
    "    data_folder = folder+'/'+str(parameters[0])+'_'+str(parameters[1])+'/'\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    # Train & save weights\n",
    "    train_loss, dev_train_loss, dev_test_loss = train_network(n_epochs, dataloader, unet, optimizer, criterion, device, data_folder)\n",
    "    \n",
    "    # Save losses\n",
    "    with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_test_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_test_loss, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
