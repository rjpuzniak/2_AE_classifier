{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torchio as tio\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchio as tio\n",
    "from torchio.transforms import (RescaleIntensity,RandomFlip,Compose, HistogramStandardization, RandomAffine, RandomNoise, ToCanonical)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "\n",
    "from Networks_Training import UNet_1_layer, UNet_2_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List IDs of all participants with albinism\n",
    "ids_albinism=['Nyst01','Nyst02','Nyst03','Nyst04','Nyst05','Nyst06','Nyst07','Nyst08','Nyst09',\n",
    "              'Nyst10','Nyst11','Nyst12','Nyst13','Nyst16','Nyst20','Nyst21','Nyst24', 'Nyst25',\n",
    "              'Nyst31','Nyst35','Nyst37','Nyst43','Nyst45','ALB1','ALB2','ALB3','ALB4',\n",
    "              'ALB5','ALB6','ALB7','ALB8','ALB9'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time creation of dictionary with listed ids of participants. It's fixed througout the experiment to better control for data poisoning\n",
    "\n",
    "datasets=['ABIDE','Athletes','HCP','COBRE','Leipzig','UoN','CHIASM','MCIC']\n",
    "\n",
    "subjects_dict={}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if dataset in ['ABIDE','Athletes','HCP','COBRE','Leipzig','MCIC']:\n",
    "       \n",
    "        ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+dataset+'/*/mask_optic_chiasm.nii.gz')]\n",
    "        random.shuffle(ids)\n",
    "    \n",
    "        subjects_dict[dataset]={}\n",
    "        subjects_dict[dataset]['control']=ids\n",
    "    \n",
    "    if dataset in ['CHIASM','UoN']:\n",
    "        \n",
    "        ids_con = ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+dataset+'/*/mask_optic_chiasm.nii.gz') if path.split('/')[-2] not in ids_albinism]\n",
    "        ids_alb = ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+dataset+'/*/mask_optic_chiasm.nii.gz') if path.split('/')[-2] in ids_albinism]\n",
    "        \n",
    "        random.shuffle(ids_con)\n",
    "        random.shuffle(ids_alb)\n",
    "        \n",
    "        subjects_dict[dataset]={}\n",
    "        subjects_dict[dataset]['control']=ids_con\n",
    "        subjects_dict[dataset]['albinism']=ids_alb\n",
    "    \n",
    "#print(subjects_dict)\n",
    "\n",
    "# Save the dictionary storing all the ids in fixed (beforehand randomized) order\n",
    "#with open('../subjects_dict.pkl', 'wb') as f:\n",
    "#    pickle.dump(subjects_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the general dictionary \n",
    "with open('../subjects_dict.pkl', 'rb') as f:\n",
    "    subjects_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total=0\n",
    "#\n",
    "#for dataset in subjects_dict.keys():\n",
    "#    for label in subjects_dict[dataset].keys():\n",
    "#        print(dataset,label,len(subjects_dict[dataset][label]))\n",
    "#        \n",
    "#        total+=len(subjects_dict[dataset][label])\n",
    "#        \n",
    "#print('total',total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the participants to train/dev_train/dev_test/test groups for the purpose of AE training\n",
    "'''\n",
    "split_training=[0.0,0.8,0.9,1.0,1.0]\n",
    "split_testing=[0.0,0.0,0.0,0.15,1.0]\n",
    "\n",
    "groups=['train','dev_train', 'dev_test', 'test']\n",
    "\n",
    "design_ae_training={}\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    \n",
    "    if i==3:\n",
    "        continue\n",
    "    else:\n",
    "        design_ae_training[groups[i]]={}\n",
    "\n",
    "        for dataset in ['ABIDE','Athletes','HCP','COBRE','Leipzig','MCIC']:\n",
    "\n",
    "            design_ae_training[groups[i]][dataset]={}\n",
    "\n",
    "            number_participants = len(subjects_dict[dataset]['control'])\n",
    "            start = np.int(np.floor(number_participants*split_training[i]))\n",
    "            end = np.int(np.floor(number_participants*split_training[i+1]))\n",
    "\n",
    "            #print(groups[i],dataset,len(subjects_dict[dataset]['control'][start:end]))\n",
    "            design_ae_training[groups[i]][dataset]['control']=subjects_dict[dataset]['control'][start:end]\n",
    "                       \n",
    "# Do the same for test\n",
    "for i in [2,3]:\n",
    "    \n",
    "    if i==3:\n",
    "        design_ae_training[groups[i]]={}\n",
    "    \n",
    "    for dataset in ['CHIASM','UoN']:\n",
    "            \n",
    "        design_ae_training[groups[i]][dataset]={}\n",
    "        \n",
    "        for label in ['control','albinism']:\n",
    "            \n",
    "            design_ae_training[groups[i]][dataset][label]={}\n",
    "\n",
    "            number_participants = len(subjects_dict[dataset][label])\n",
    "            start = np.int(np.floor(number_participants*split_testing[i]))\n",
    "            end = np.int(np.floor(number_participants*split_testing[i+1]))\n",
    "\n",
    "            #print(groups[i],dataset,label,len(subjects_dict[dataset][label][start:end]))\n",
    "            design_ae_training[groups[i]][dataset][label]=subjects_dict[dataset][label][start:end]\n",
    "'''            \n",
    "# Check the number of participants in each group\n",
    "\n",
    "#for a in design_ae_training.keys():\n",
    "#    print('\\n')\n",
    "#    for b in design_ae_training[a].keys():\n",
    "#        for c in design_ae_training[a][b].keys():\n",
    "#            print(a,b,c, len(design_ae_training[a][b][c]))\n",
    "\n",
    "# Save\n",
    "#with open('design_ae_training.pkl', 'wb') as f:\n",
    "#    pickle.dump(design_ae_training, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary for AE training \n",
    "with open('design_ae_training.pkl', 'rb') as f:\n",
    "    design_ae_training = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with data required for creation of dataset and dataloader\n",
    "dict_ae_training=copy.deepcopy(design_ae_training)\n",
    "\n",
    "'''\n",
    "# train and dev_train (combined, randomized data)\n",
    "for group in ['train','dev_train']:\n",
    "    \n",
    "    all_subjects=[]\n",
    "    \n",
    "    for dataset in design_ae_training[group].keys():\n",
    "        for label in design_ae_training[group][dataset].keys():\n",
    "                        \n",
    "            all_subjects+=[tio.Subject(t1=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/t1w_1mm_iso_brain.nii.gz', type = tio.INTENSITY),\n",
    "                                        probs=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/sampling_distribution.nii.gz', type = tio.INTENSITY)) \n",
    "                                        for subject in design_ae_training[group][dataset][label]]\n",
    "\n",
    "    dict_ae_training[group]=all_subjects\n",
    "'''           \n",
    "for group in design_ae_training.keys():\n",
    "    for dataset in design_ae_training[group].keys():\n",
    "        for label in design_ae_training[group][dataset].keys():\n",
    "                        \n",
    "            dict_ae_training[group][dataset][label]=[tio.Subject(t1=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/t1w_1mm_iso_brain.nii.gz', type = tio.INTENSITY),\n",
    "                                                              probs=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/sampling_distribution.nii.gz', type = tio.INTENSITY)) \n",
    "                                                  for subject in design_ae_training[group][dataset][label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [09:44<00:00,  2.98it/s]\n",
      "100%|██████████| 1742/1742 [06:52<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Histogram standardization (to mitigate cross-site differences)\n",
    "# Standardization is performed on all datasets\n",
    "\n",
    "# Save paths of all images\n",
    "images_paths=[]\n",
    "probs_paths=[]\n",
    "\n",
    "for group in design_ae_training.keys():\n",
    "    for dataset in design_ae_training[group].keys():\n",
    "        for label in design_ae_training[group][dataset].keys():\n",
    "            for subject in design_ae_training[group][dataset][label]:\n",
    "                images_paths.append('../../1_Data/1_Input/'+dataset+'/'+subject+'/t1w_1mm_iso_brain.nii.gz')\n",
    "                probs_paths.append('../../1_Data/1_Input/'+dataset+'/'+subject+'/sampling_distribution.nii.gz')\n",
    "\n",
    "images_landmarks_paths = Path('images_landmarks.npy') \n",
    "probs_landmarks_paths = Path('probs_landmarks.npy') \n",
    "\n",
    "images_landmarks = HistogramStandardization.train(images_paths)\n",
    "probs_landmarks = HistogramStandardization.train(probs_paths)\n",
    "\n",
    "torch.save(images_landmarks, images_landmarks_paths)\n",
    "torch.save(probs_landmarks, probs_landmarks_paths)\n",
    "\n",
    "landmarks={'t1': images_landmarks,\n",
    "          'probs': probs_landmarks}\n",
    "\n",
    "standardize = HistogramStandardization(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "# Rescale\n",
    "rescale = RescaleIntensity((0,1))\n",
    "\n",
    "# Flip\n",
    "flip = RandomFlip((0,1,2), flip_probability=0.5, p=0.25)\n",
    "\n",
    "# Composing transforms \n",
    "transform_train = Compose([standardize, rescale, flip]) \n",
    "transform_dev = Compose([standardize, rescale]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Torchio dataset\n",
    "\n",
    "dataset_ae_training = {}\n",
    "\n",
    "\n",
    "# train\n",
    "#dataset_ae_training['train']=tio.SubjectsDataset(dict_ae_training['train'], transform=transform_train)\n",
    "          \n",
    "# dev_train\n",
    "#dataset_ae_training['dev_train']=tio.SubjectsDataset(dict_ae_training['dev_train'], transform=transform_dev)\n",
    "    \n",
    "# dev_test and test\n",
    "for group in dict_ae_training.keys():\n",
    "    \n",
    "    dataset_ae_training[group]={}\n",
    "    \n",
    "    for dataset in dict_ae_training[group].keys():\n",
    "        \n",
    "        dataset_ae_training[group][dataset]={}\n",
    "            \n",
    "        for label in dict_ae_training[group][dataset].keys():\n",
    "            \n",
    "            dataset_ae_training[group][dataset][label]=tio.SubjectsDataset(dict_ae_training[group][dataset][label], transform=transform_dev)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler\n",
    "patch_size = (24,24,8)\n",
    "queue_length = 200\n",
    "samples_per_volume = 5\n",
    "\n",
    "sampler = tio.data.WeightedSampler(patch_size,'probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and dev_train datasets (dev_test and test remain as they are)\n",
    "concatenated_datasets={}\n",
    "\n",
    "for group in ['train','dev_train']:\n",
    "\n",
    "    entry=[]\n",
    "    \n",
    "    for dataset in dataset_ae_training[group].keys():\n",
    "    \n",
    "        for labels in dataset_ae_training[group][dataset].keys():\n",
    "            \n",
    "            entry.append(dataset_ae_training[group][dataset][labels])\n",
    "   \n",
    "    #print(entry)\n",
    "    concatenated_datasets[group]=torch.utils.data.ConcatDataset(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor group in ['dev_test','test']:\\n    dataloader[group]={}\\n    for dataset in dataset_ae_training[group].keys():\\n        dataloader[group][dataset]={}\\n        for label in dataset_ae_training[group][dataset].keys():\\n            dataloader[group][dataset][label]=DataLoader(tio.Queue(dataset_ae_training[group][dataset][label], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size = 25, num_workers=0)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataloader\n",
    "\n",
    "dataloader = {}\n",
    "\n",
    "# train & dev_train\n",
    "dataloader['train']= DataLoader(tio.Queue(concatenated_datasets['train'], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size=20, num_workers=0)\n",
    "dataloader['dev_train']= DataLoader(tio.Queue(concatenated_datasets['dev_train'], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size=20, num_workers=0)\n",
    "\n",
    "# dev_test and test\n",
    "'''\n",
    "for group in ['dev_test','test']:\n",
    "    dataloader[group]={}\n",
    "    for dataset in dataset_ae_training[group].keys():\n",
    "        dataloader[group][dataset]={}\n",
    "        for label in dataset_ae_training[group][dataset].keys():\n",
    "            dataloader[group][dataset][label]=DataLoader(tio.Queue(dataset_ae_training[group][dataset][label], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size = 25, num_workers=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#model = torch.nn.Identity()\n",
    "\n",
    "#for patches_batch in dataloader['dev_test']['MCIC']['control']:\n",
    "    #print(patches_batch)\n",
    "#    inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\n",
    "#    targets = patches_batch['t1'][tio.DATA]  # key 'brain' is in subject\n",
    "#    logits = model(inputs)  # model being an instance of torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.shape\n",
    "\n",
    "#fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "#for i in range(inputs.shape[0]):\n",
    "#    plt.subplot(5,8,i+1)\n",
    "#    plt.imshow(inputs[i,0,:,:,5],cmap='gray');\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Try setting CUDA if possible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") \n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "#criterion = DiceLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning trained model\n",
    "def train_network(n_epochs, dataloaders, model, optimizer, criterion, device, save_path):\n",
    "    \n",
    "    track_train_loss = []\n",
    "    track_dev_train_loss = []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        # Initialize loss monitoring variables\n",
    "        train_loss = 0.0\n",
    "        dev_train_loss = 0.0\n",
    "        \n",
    "        i=0\n",
    "        j=0\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        for batch in dataloaders['train']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            i+=1\n",
    "            \n",
    "        track_train_loss.append(train_loss/i)\n",
    "        \n",
    "        # dev_train\n",
    "        model.eval()\n",
    "        \n",
    "        for batch in dataloaders['dev_train']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                dev_train_loss += loss.item()\n",
    "                j+=1\n",
    "                \n",
    "        track_dev_train_loss.append(dev_train_loss/j)\n",
    "\n",
    "        print('END OF EPOCH: {} \\tTraining loss per batch: {:.6f}\\tTraining_dev loss per image: {:.6f}'.format(epoch, train_loss/i, dev_train_loss/j))\n",
    "           \n",
    "        ## Save the model if reached min validation loss\n",
    "        if dev_train_loss  < valid_loss_min:\n",
    "            valid_loss_min = dev_train_loss\n",
    "            torch.save(model.state_dict(),save_path+'optimal_weights')\n",
    "            last_updated_epoch = epoch\n",
    "            \n",
    "            with open(save_path+'number_epochs.txt','w') as f:\n",
    "                print('Epoch:', str(epoch), file=f)\n",
    "                \n",
    "        # Early stopping\n",
    "        if (epoch - last_updated_epoch) == 5:\n",
    "            break\n",
    "                        \n",
    "    # return trained model\n",
    "    return track_train_loss, track_dev_train_loss         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 1/40 [11:14<7:18:33, 674.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.013790\tTraining_dev loss per image: 0.003050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/40 [21:05<6:51:22, 649.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004300\tTraining_dev loss per image: 0.003866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/40 [30:54<6:29:25, 631.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.003892\tTraining_dev loss per image: 0.003153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/40 [40:46<6:11:43, 619.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.003634\tTraining_dev loss per image: 0.003057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 5/40 [50:35<5:56:07, 610.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.003564\tTraining_dev loss per image: 0.002573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/40 [1:00:56<5:47:34, 613.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003535\tTraining_dev loss per image: 0.002634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/40 [1:10:48<5:33:53, 607.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.003409\tTraining_dev loss per image: 0.003134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 8/40 [1:20:36<5:20:43, 601.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003276\tTraining_dev loss per image: 0.002784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▎       | 9/40 [1:30:40<5:11:07, 602.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003357\tTraining_dev loss per image: 0.002936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 10/40 [1:40:33<4:59:40, 599.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003476\tTraining_dev loss per image: 0.002488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/40 [1:50:25<4:48:36, 597.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003385\tTraining_dev loss per image: 0.003654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 12/40 [2:00:27<4:39:22, 598.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003485\tTraining_dev loss per image: 0.002210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▎      | 13/40 [2:10:18<4:28:24, 596.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003245\tTraining_dev loss per image: 0.003237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 14/40 [2:20:06<4:17:21, 593.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003335\tTraining_dev loss per image: 0.004479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 15/40 [2:29:57<4:07:05, 593.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003374\tTraining_dev loss per image: 0.002609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 16/40 [2:39:46<3:56:44, 591.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003330\tTraining_dev loss per image: 0.002777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▎     | 17/40 [2:49:52<3:48:31, 596.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003302\tTraining_dev loss per image: 0.002187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 18/40 [2:59:56<3:39:23, 598.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003280\tTraining_dev loss per image: 0.002699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 19/40 [3:09:45<3:28:24, 595.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.003265\tTraining_dev loss per image: 0.002605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 20/40 [3:19:34<3:17:52, 593.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003169\tTraining_dev loss per image: 0.002218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▎    | 21/40 [3:29:25<3:07:45, 592.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.003237\tTraining_dev loss per image: 0.002835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▎    | 21/40 [3:39:17<3:18:24, 626.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.003259\tTraining_dev loss per image: 0.002294\n",
      "[2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▎         | 1/40 [10:05<6:33:40, 605.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.036390\tTraining_dev loss per image: 0.018867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/40 [19:55<6:20:32, 600.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.012927\tTraining_dev loss per image: 0.008364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/40 [30:00<6:11:21, 602.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.007601\tTraining_dev loss per image: 0.005723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/40 [39:51<5:59:15, 598.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.005850\tTraining_dev loss per image: 0.004133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 5/40 [49:42<5:47:51, 596.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.005151\tTraining_dev loss per image: 0.006470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/40 [59:30<5:36:36, 594.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004635\tTraining_dev loss per image: 0.003179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/40 [1:09:31<5:27:51, 596.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004244\tTraining_dev loss per image: 0.003128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 8/40 [1:19:22<5:17:08, 594.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003979\tTraining_dev loss per image: 0.003118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▎       | 9/40 [1:29:13<5:06:34, 593.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.003656\tTraining_dev loss per image: 0.004680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 10/40 [1:38:59<4:55:39, 591.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003549\tTraining_dev loss per image: 0.002409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/40 [1:49:05<4:47:56, 595.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.003454\tTraining_dev loss per image: 0.002330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 12/40 [1:58:58<4:37:31, 594.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003336\tTraining_dev loss per image: 0.002963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▎      | 13/40 [2:09:00<4:28:39, 597.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003118\tTraining_dev loss per image: 0.002060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 14/40 [2:18:51<4:17:53, 595.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.002923\tTraining_dev loss per image: 0.001906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 15/40 [2:28:38<4:07:00, 592.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003065\tTraining_dev loss per image: 0.002175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 16/40 [2:38:32<3:57:13, 593.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.002844\tTraining_dev loss per image: 0.004322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▎     | 17/40 [2:48:21<3:46:54, 591.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.002958\tTraining_dev loss per image: 0.001907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 18/40 [2:58:06<3:36:12, 589.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003040\tTraining_dev loss per image: 0.002330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 18/40 [3:08:26<3:50:19, 628.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.002893\tTraining_dev loss per image: 0.002293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training for 1-layer network\n",
    "\n",
    "model_parameters=[[4,1],[2,2]]\n",
    "\n",
    "folder='../../1_Data/2_Trained_AE/'\n",
    "\n",
    "for parameters in model_parameters:\n",
    "    \n",
    "    print(parameters)\n",
    "        \n",
    "    # Initialize the proper model\n",
    "    unet = UNet_1_layer(1,1,parameters[0],parameters[1])\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(params=unet.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create output folder\n",
    "    data_folder = folder+'/1_layer_'+str(parameters[0])+'_'+str(parameters[1])+'/'\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    # Train & save weights\n",
    "    train_loss, dev_train_loss = train_network(n_epochs, dataloader, unet, optimizer, criterion, device, data_folder)\n",
    "    \n",
    "    # Save losses\n",
    "    with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_train_loss, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 1/40 [09:51<6:24:38, 591.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.010423\tTraining_dev loss per image: 0.004013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/40 [19:43<6:14:50, 591.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.005744\tTraining_dev loss per image: 0.005842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/40 [29:33<6:04:28, 591.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005356\tTraining_dev loss per image: 0.005457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/40 [39:31<5:56:01, 593.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004780\tTraining_dev loss per image: 0.004222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 5/40 [49:37<5:48:21, 597.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004324\tTraining_dev loss per image: 0.004246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/40 [59:28<5:37:13, 595.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004343\tTraining_dev loss per image: 0.003693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/40 [1:09:14<5:25:55, 592.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004385\tTraining_dev loss per image: 0.003894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 8/40 [1:19:19<5:18:03, 596.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004158\tTraining_dev loss per image: 0.003161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▎       | 9/40 [1:29:08<5:06:57, 594.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.004174\tTraining_dev loss per image: 0.003592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 10/40 [1:39:00<4:56:45, 593.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.003951\tTraining_dev loss per image: 0.003575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/40 [1:49:06<4:48:34, 597.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.004004\tTraining_dev loss per image: 0.003293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 12/40 [1:58:54<4:37:24, 594.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003918\tTraining_dev loss per image: 0.003558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [2:08:45<5:00:26, 643.80s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003878\tTraining_dev loss per image: 0.003623\n",
      "[32, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 1/40 [09:50<6:23:52, 590.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.009584\tTraining_dev loss per image: 0.004991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/40 [19:38<6:13:29, 589.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.005560\tTraining_dev loss per image: 0.004536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/40 [29:43<6:06:32, 594.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005314\tTraining_dev loss per image: 0.004722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/40 [39:48<5:58:30, 597.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.005083\tTraining_dev loss per image: 0.003851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 5/40 [49:38<5:47:17, 595.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004786\tTraining_dev loss per image: 0.003976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/40 [59:30<5:36:42, 594.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004568\tTraining_dev loss per image: 0.003198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/40 [1:09:18<5:25:49, 592.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004299\tTraining_dev loss per image: 0.003526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 8/40 [1:19:07<5:15:21, 591.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004015\tTraining_dev loss per image: 0.003110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▎       | 9/40 [1:29:08<5:07:06, 594.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.004133\tTraining_dev loss per image: 0.003044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 10/40 [1:38:58<4:56:28, 592.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.004009\tTraining_dev loss per image: 0.002720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/40 [1:49:04<4:48:26, 596.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.004132\tTraining_dev loss per image: 0.002974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 12/40 [1:58:50<4:37:01, 593.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.003720\tTraining_dev loss per image: 0.002962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▎      | 13/40 [2:08:41<4:26:46, 592.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.003866\tTraining_dev loss per image: 0.005111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 14/40 [2:18:31<4:16:33, 592.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.003869\tTraining_dev loss per image: 0.002767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [2:28:33<4:35:54, 636.71s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.003727\tTraining_dev loss per image: 0.003477\n",
      "[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 1/40 [09:48<6:22:35, 588.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.019419\tTraining_dev loss per image: 0.008078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/40 [19:39<6:13:07, 589.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.005747\tTraining_dev loss per image: 0.004269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/40 [29:29<6:03:31, 589.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.005235\tTraining_dev loss per image: 0.004489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/40 [39:34<5:56:32, 594.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004689\tTraining_dev loss per image: 0.005386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 5/40 [49:23<5:45:45, 592.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.004577\tTraining_dev loss per image: 0.003312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/40 [59:30<5:38:17, 596.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.004503\tTraining_dev loss per image: 0.004508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/40 [1:09:19<5:27:01, 594.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.004393\tTraining_dev loss per image: 0.003641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 8/40 [1:19:10<5:16:30, 593.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.004349\tTraining_dev loss per image: 0.003514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▎       | 9/40 [1:29:00<5:06:05, 592.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.004284\tTraining_dev loss per image: 0.003294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 10/40 [1:38:50<4:55:45, 591.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.004364\tTraining_dev loss per image: 0.003213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/40 [1:48:33<4:44:39, 588.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.004061\tTraining_dev loss per image: 0.003355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 12/40 [1:58:53<4:39:18, 598.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.004174\tTraining_dev loss per image: 0.003942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▎      | 13/40 [2:08:45<4:28:23, 596.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.004101\tTraining_dev loss per image: 0.003117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 14/40 [2:18:33<4:17:23, 593.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.004000\tTraining_dev loss per image: 0.003048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 15/40 [2:28:24<4:07:04, 592.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.004015\tTraining_dev loss per image: 0.002812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 16/40 [2:38:13<3:56:44, 591.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.003853\tTraining_dev loss per image: 0.003151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▎     | 17/40 [2:48:17<3:48:14, 595.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.003905\tTraining_dev loss per image: 0.003214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 18/40 [2:58:06<3:37:40, 593.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.003879\tTraining_dev loss per image: 0.003089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 19/40 [3:07:53<3:27:03, 591.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.004026\tTraining_dev loss per image: 0.004015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 20/40 [3:18:00<3:18:41, 596.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.003913\tTraining_dev loss per image: 0.002650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▎    | 21/40 [3:27:48<3:08:00, 593.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.003870\tTraining_dev loss per image: 0.007027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 22/40 [3:37:38<2:57:49, 592.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.003851\tTraining_dev loss per image: 0.002907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▊    | 23/40 [3:47:42<2:48:52, 596.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.003734\tTraining_dev loss per image: 0.002847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 24/40 [3:57:32<2:38:25, 594.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.003749\tTraining_dev loss per image: 0.002980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 24/40 [4:07:21<2:44:54, 618.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.003663\tTraining_dev loss per image: 0.003164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training for 2-layer network\n",
    "model_parameters=[[8,2],[32,1]]\n",
    "\n",
    "folder='../../1_Data/2_Trained_AE/'\n",
    "\n",
    "for parameters in model_parameters:\n",
    "    \n",
    "    print(parameters)\n",
    "        \n",
    "    # Initialize the proper model\n",
    "    unet = UNet_2_layer(1,1,parameters[0],parameters[1])\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(params=unet.parameters(), lr=0.0025)\n",
    "    \n",
    "    # Create output folder\n",
    "    data_folder = folder+'/2_layer_'+str(parameters[0])+'_'+str(parameters[1])+'/'\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    # Train & save weights\n",
    "    train_loss, dev_train_loss = train_network(n_epochs, dataloader, unet, optimizer, criterion, device, data_folder)\n",
    "    \n",
    "    # Save losses\n",
    "    with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_train_loss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
