{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torchio as tio\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchio as tio\n",
    "from torchio.transforms import (RescaleIntensity,RandomFlip,Compose, HistogramStandardization, RandomAffine, RandomNoise, ToCanonical)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary describing assignment of participants to the groups\n",
    "\n",
    "# Groups\n",
    "train_groups=['ABIDE','Athletes','HCP','COBRE','Leipzig']\n",
    "test_dev_groups=['UoN']\n",
    "test_groups=['CHIASM']\n",
    "\n",
    "# CHIASM dobry\n",
    "# ABIDE dobry\n",
    "# Athletes dobry\n",
    "# COBRE dobry\n",
    "# HCP dobry\n",
    "# Leipzig good\n",
    "# MCIC z≈Çe\n",
    "# UoN dobry\n",
    "\n",
    "# Splits\n",
    "train_split = 0.85\n",
    "dev_split = 0.15\n",
    "test_split = 0.0\n",
    "\n",
    "# Dictionary with study design\n",
    "design = {}\n",
    "\n",
    "design['train']={}\n",
    "design['dev_train']={}\n",
    "design['dev_test']={}\n",
    "design['test']={}\n",
    "\n",
    "design['train']['all']={}\n",
    "design['dev_train']['all']={}\n",
    "design['dev_test']['all']={}\n",
    "design['test']['all']={}\n",
    "\n",
    "# Training data\n",
    "for group in train_groups:\n",
    "    \n",
    "    #design['dev_train'][group]={}\n",
    "    #design['test'][group]={}\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    # Randomize order\n",
    "    random.shuffle(ids) \n",
    "    # Find split ratios\n",
    "    train_idx = np.int(np.floor(len(ids)*train_split))\n",
    "    dev_idx = np.int(np.floor(len(ids)*(train_split+dev_split)))\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+ids[i]+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "    \n",
    "        if i+1<=train_idx:\n",
    "            design['train']['all'][ids[i]]=files\n",
    "        elif i+1 > dev_idx:\n",
    "            design['test']['all'][ids[i]]=files\n",
    "            #design['test'][group][ids[i]]=files\n",
    "        else:\n",
    "            design['dev_train']['all'][ids[i]]=files\n",
    "            #design['dev_train'][group][ids[i]]=files\n",
    "        \n",
    "# Dev data\n",
    "for group in test_dev_groups:\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    \n",
    "    for sub_id in ids:\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "        \n",
    "        design['dev_test']['all'][sub_id]=files\n",
    "    \n",
    "# Test data\n",
    "for group in test_groups:\n",
    "    \n",
    "    #design['test'][group]={}\n",
    "\n",
    "    # Idices of all subjects\n",
    "    ids=[path.split('/')[-2] for path in glob.glob('../../1_Data/1_Input/'+group+'/*/tmp_brain_mask.nii.gz')] \n",
    "    \n",
    "    for sub_id in ids:\n",
    "        \n",
    "        path_to_folder='../../1_Data/1_Input/'+group+'/'+sub_id+'/'\n",
    "        \n",
    "        files={}\n",
    "        files['brain']=path_to_folder+'t1w_1mm_iso.nii.gz'\n",
    "        files['probs']=path_to_folder+'tmp_brain_mask.nii.gz'\n",
    "        files['chiasm']=path_to_folder+'chiasm.nii.gz'\n",
    "        \n",
    "        design['test']['all'][sub_id]=files \n",
    "        #design['test'][group][sub_id]=files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary\n",
    "#with open('study_design.pkl', 'wb') as f:\n",
    "#    pickle.dump(design, f)\n",
    "\n",
    "# Load the dictionary\n",
    "with open('study_design.pkl', 'rb') as f:\n",
    "    design = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all images\n",
    "subjects_list={}\n",
    "\n",
    "for group in design.keys():\n",
    "    subjects_list[group]={}\n",
    "    \n",
    "    for dataset in design[group].keys():\n",
    "        subjects_list[group][dataset]= [tio.Subject(t1=tio.Image(design[group][dataset][sub]['brain'], type=tio.INTENSITY),\n",
    "                            probs = tio.Image(design[group][dataset][sub]['probs'], type = tio.INTENSITY)) for sub in design[group][dataset].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale\n",
    "rescale = RescaleIntensity((0,1))\n",
    "# Flip\n",
    "flip = RandomFlip((0,1,2), flip_probability=0.5, p=0.25)\n",
    "# Affine transformations\n",
    "#affine = RandomAffine(degrees=30)\n",
    "\n",
    "# Composing transforms - rescaling is mandatory, training data is subjected to a range of additional augmentations\n",
    "transform_train = Compose([rescale, flip]) # leaving out standardization for now\n",
    "transform_dev = Compose([rescale]) # leaving out standardization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchio's (Pytorch's) Dataset\n",
    "data = {}\n",
    "\n",
    "for group in subjects_list.keys():\n",
    "    data[group]={}\n",
    "    \n",
    "    for dataset in subjects_list[group]:\n",
    "        \n",
    "        if group == 'train':\n",
    "            data[group][dataset]=tio.SubjectsDataset(subjects_list[group][dataset], transform=transform_train)\n",
    "        else:\n",
    "            data[group][dataset]=tio.SubjectsDataset(subjects_list[group][dataset], transform=transform_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler\n",
    "patch_size = (24,24,8)\n",
    "queue_length = 500\n",
    "samples_per_volume = 5\n",
    "\n",
    "sampler = tio.data.WeightedSampler(patch_size,'probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "dataloader = {}\n",
    "\n",
    "for group in data.keys():\n",
    "    dataloader[group]={}\n",
    "    \n",
    "    for dataset in data[group]:\n",
    "        \n",
    "        dataloader[group][dataset]=DataLoader(tio.Queue(data[group][dataset], queue_length, samples_per_volume, sampler, num_workers=6, shuffle_subjects=True, shuffle_patches=True), batch_size = 25, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_epochs = 1\\n\\nmodel = torch.nn.Identity()\\n\\nfor epoch_index in range(num_epochs):\\n    for patches_batch in dataloader['dev_train']['all']:\\n        #print(patches_batch)\\n        inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\\n        targets = patches_batch['t1'][tio.DATA]  # key 'brain' is in subject\\n        logits = model(inputs)  # model being an instance of torch.nn.Module\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "'''\n",
    "num_epochs = 1\n",
    "\n",
    "model = torch.nn.Identity()\n",
    "\n",
    "for epoch_index in range(num_epochs):\n",
    "    for patches_batch in dataloader['dev_train']['all']:\n",
    "        #print(patches_batch)\n",
    "        inputs = patches_batch['t1'][tio.DATA]  # key 't1' is in subject\n",
    "        targets = patches_batch['t1'][tio.DATA]  # key 'brain' is in subject\n",
    "        logits = model(inputs)  # model being an instance of torch.nn.Module\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfig = plt.figure(figsize=(20, 10))\\n\\nfor i in range(inputs.shape[0]):\\n    plt.subplot(5,8,i+1)\\n    plt.imshow(inputs[i,0,:,:,5],cmap='gray');\\n    \\nplt.show()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs.shape\n",
    "'''\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(inputs.shape[0]):\n",
    "    plt.subplot(5,8,i+1)\n",
    "    plt.imshow(inputs[i,0,:,:,5],cmap='gray');\n",
    "    \n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torchio as tio\\nt1 = tio.ScalarImage('T1w')\\nt2 = tio.ScalarImage('T2w')\\nsubject = tio.Subject(T1w=t1, T2w=t2)\\ncp = tio.CropOrPad((512, 512, 408))\\nsubject = tio.Subject(T1w=cp(t1), T2w=cp(t2))\\nsubject.plot(reorient=False)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torchio as tio\n",
    "t1 = tio.ScalarImage('T1w')\n",
    "t2 = tio.ScalarImage('T2w')\n",
    "subject = tio.Subject(T1w=t1, T2w=t2)\n",
    "cp = tio.CropOrPad((512, 512, 408))\n",
    "subject = tio.Subject(T1w=cp(t1), T2w=cp(t2))\n",
    "subject.plot(reorient=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Try setting CUDA if possible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") \n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropped U-Net copied from Overfitting Model\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=10, scaling=2):\n",
    "        super(UNet, self).__init__()\n",
    "                \n",
    "        # Encoding layers\n",
    "        self.encoder1 = self.unet_block(in_channels, init_features, \"enc1\")\n",
    "        self.pool1 = nn.AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.encoder2 = self.unet_block(init_features, init_features*scaling, name='enc2')\n",
    "        self.pool2 = nn.AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        self.bottleneck = self.unet_block(init_features*scaling, init_features*scaling**2, name='bottleneck')\n",
    "        \n",
    "        # Decoding layers (where merge with prevois encoding layers occurs)        \n",
    "        self.upconv2 = nn.ConvTranspose3d(init_features*scaling**2, init_features*scaling, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.unet_block(init_features*scaling, init_features*scaling, name='dec2')\n",
    "                \n",
    "        self.upconv1 = nn.ConvTranspose3d(init_features*scaling, init_features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.unet_block(init_features, init_features, name='dec1')\n",
    "        \n",
    "        # Final convolution - output equals number of output channels\n",
    "        self.conv = nn.Conv3d(init_features, out_channels, kernel_size=1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        # Upconvolving, concatenating data from respective encoding phase and executing UNet block\n",
    "        dec2 = self.upconv2(bottleneck)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        out_conv = self.conv(dec1)\n",
    "        \n",
    "        return torch.sigmoid(out_conv)\n",
    "    \n",
    "    def unet_block(self, in_channels, features, name):\n",
    "        \n",
    "        return nn.Sequential(OrderedDict([(name+'conv1',nn.Conv3d(in_channels=in_channels, out_channels=features, kernel_size=3, padding=1, bias=False)),\n",
    "                             (name+'bnorm1', nn.BatchNorm3d(num_features=features)),\n",
    "                             (name+'relu1', nn.ReLU(inplace=True)),\n",
    "                             (name+'conv2', nn.Conv3d(in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False)),\n",
    "                             (name+'bnorm2', nn.BatchNorm3d(num_features=features)),\n",
    "                             (name+'relu2', nn.ReLU(inplace=True))])\n",
    "                            )\n",
    "\n",
    "    def output_latent_representations(self,x):\n",
    "        \n",
    "        print(x.shape)\n",
    "\n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "        \n",
    "        print(bottleneck.shape)\n",
    "        \n",
    "        return bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet = UNet(1,1,2,2) # Size of latent representation = (1/64) * init_features * scaling**2\n",
    "#unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sum(p.numel() for p in unet.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noutputs = unet(inputs.to(device))\\noutputs = outputs.cpu().detach().numpy()\\n\\nfig = plt.figure(figsize=(20, 10))\\n\\nfor i in range(outputs.shape[0]):\\n    plt.subplot(10,10,i+1)\\n    plt.imshow(outputs[i,0,:,:,5],cmap='gray');\\n    \\nplt.show()\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try processing\n",
    "'''\n",
    "outputs = unet(inputs.to(device))\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(outputs.shape[0]):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(outputs[i,0,:,:,5],cmap='gray');\n",
    "    \n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "#criterion = DiceLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcome = criterion(inputs, torch.Tensor(outputs))\n",
    "#print(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning trained model\n",
    "def train_network(n_epochs, dataloaders, model, optimizer, criterion, device, save_path):\n",
    "    \n",
    "    track_train_loss = []\n",
    "    track_dev_train_loss = []\n",
    "    track_dev_test_loss = []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        # Initialize loss monitoring variables\n",
    "        train_loss = 0.0\n",
    "        dev_train_loss = 0.0\n",
    "        dev_test_loss = 0.0\n",
    "        \n",
    "        i=0\n",
    "        k=0\n",
    "        j=0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        \n",
    "        for batch in dataloaders['train']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            i+=1\n",
    "            \n",
    "        track_train_loss.append(train_loss/i)\n",
    "        \n",
    "        # Validation on two datasets\n",
    "        model.eval()\n",
    "        \n",
    "        for batch in dataloaders['dev_train']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                dev_train_loss += loss.item()\n",
    "                j+=1\n",
    "                \n",
    "        track_dev_train_loss.append(dev_train_loss/j)\n",
    "        \n",
    "        \n",
    "        for batch in dataloaders['dev_test']['all']:\n",
    "            \n",
    "            data = batch['t1']['data'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                dev_test_loss += loss.item()\n",
    "                k+=1\n",
    "                \n",
    "        track_dev_test_loss.append(dev_test_loss/k)\n",
    "        \n",
    "        # Print summary of epoch\n",
    "        duration = time.time() - start\n",
    "\n",
    "        print('END OF EPOCH: {} \\tTraining loss per batch: {:.6f}\\tTraining_dev loss per batch: {:.6f}\\tTest_dev loss per batch: {:.6f}'.format(epoch, train_loss/i, dev_train_loss/j, dev_test_loss/k))\n",
    "       \n",
    "        \n",
    "        ## Save the model if reached min validation loss\n",
    "        if dev_train_loss + dev_test_loss < valid_loss_min:\n",
    "            valid_loss_min = dev_train_loss + dev_test_loss\n",
    "            torch.save(model.state_dict(),save_path+'optimal_weights')\n",
    "                        \n",
    "    # return trained model\n",
    "    return track_train_loss, track_dev_train_loss, track_dev_test_loss         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 1/25 [33:27<13:22:54, 2007.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.021654\tTraining_dev loss per batch: 0.009881\tTest_dev loss per batch: 0.008477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|‚ñä         | 2/25 [1:03:47<12:27:55, 1951.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.006293\tTraining_dev loss per batch: 0.004711\tTest_dev loss per batch: 0.003947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|‚ñà‚ñè        | 3/25 [1:33:15<11:35:18, 1896.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.006213\tTraining_dev loss per batch: 0.004244\tTest_dev loss per batch: 0.002689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|‚ñà‚ñå        | 4/25 [2:04:17<11:00:01, 1885.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 4 \tTraining loss per batch: 0.005990\tTraining_dev loss per batch: 0.004316\tTest_dev loss per batch: 0.002692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|‚ñà‚ñà        | 5/25 [2:33:31<10:15:30, 1846.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.006079\tTraining_dev loss per batch: 0.004398\tTest_dev loss per batch: 0.002063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|‚ñà‚ñà‚ñç       | 6/25 [3:03:06<9:37:55, 1825.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.005818\tTraining_dev loss per batch: 0.004326\tTest_dev loss per batch: 0.002290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|‚ñà‚ñà‚ñä       | 7/25 [3:34:49<9:14:32, 1848.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.005797\tTraining_dev loss per batch: 0.004415\tTest_dev loss per batch: 0.001974\n",
      "END OF EPOCH: 8 \tTraining loss per batch: 0.005939\tTraining_dev loss per batch: 0.004125\tTest_dev loss per batch: 0.002412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [4:39:08<8:23:01, 1886.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 9 \tTraining loss per batch: 0.005888\tTraining_dev loss per batch: 0.005010\tTest_dev loss per batch: 0.002049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [5:11:32<7:55:52, 1903.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.005790\tTraining_dev loss per batch: 0.004245\tTest_dev loss per batch: 0.002489\n",
      "END OF EPOCH: 11 \tTraining loss per batch: 0.005830\tTraining_dev loss per batch: 0.004184\tTest_dev loss per batch: 0.001751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [6:13:04<6:46:02, 1874.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.005925\tTraining_dev loss per batch: 0.004099\tTest_dev loss per batch: 0.001886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [6:43:07<6:10:33, 1852.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.005597\tTraining_dev loss per batch: 0.004163\tTest_dev loss per batch: 0.002226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [7:13:10<5:36:55, 1837.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 14 \tTraining loss per batch: 0.005876\tTraining_dev loss per batch: 0.004321\tTest_dev loss per batch: 0.001676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [7:43:19<5:04:50, 1829.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.005710\tTraining_dev loss per batch: 0.004663\tTest_dev loss per batch: 0.001992\n",
      "END OF EPOCH: 16 \tTraining loss per batch: 0.006005\tTraining_dev loss per batch: 0.003908\tTest_dev loss per batch: 0.001818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [8:43:37<4:02:28, 1818.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.006151\tTraining_dev loss per batch: 0.004062\tTest_dev loss per batch: 0.001745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [9:13:30<3:31:15, 1810.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 18 \tTraining loss per batch: 0.005835\tTraining_dev loss per batch: 0.004259\tTest_dev loss per batch: 0.002180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [9:46:01<3:05:16, 1852.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.005988\tTraining_dev loss per batch: 0.003973\tTest_dev loss per batch: 0.002087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [10:14:12<2:30:21, 1804.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 20 \tTraining loss per batch: 0.005988\tTraining_dev loss per batch: 0.004068\tTest_dev loss per batch: 0.001753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [10:42:55<1:58:40, 1780.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.005896\tTraining_dev loss per batch: 0.003974\tTest_dev loss per batch: 0.002085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [11:11:45<1:28:15, 1765.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.005845\tTraining_dev loss per batch: 0.004052\tTest_dev loss per batch: 0.001973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [11:40:36<58:29, 1754.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 23 \tTraining loss per batch: 0.005489\tTraining_dev loss per batch: 0.004083\tTest_dev loss per batch: 0.001988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [12:09:30<29:08, 1748.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.005861\tTraining_dev loss per batch: 0.004169\tTest_dev loss per batch: 0.001916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [12:38:31<00:00, 1820.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.005777\tTraining_dev loss per batch: 0.004529\tTest_dev loss per batch: 0.001997\n",
      "[32, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1 \tTraining loss per batch: 0.006281\tTraining_dev loss per batch: 0.003072\tTest_dev loss per batch: 0.000938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 2/25 [57:36<11:02:10, 1727.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2 \tTraining loss per batch: 0.004664\tTraining_dev loss per batch: 0.003160\tTest_dev loss per batch: 0.000709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|‚ñà‚ñè        | 3/25 [1:26:45<10:35:46, 1733.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 3 \tTraining loss per batch: 0.004152\tTraining_dev loss per batch: 0.005915\tTest_dev loss per batch: 0.000950\n",
      "END OF EPOCH: 4 \tTraining loss per batch: 0.004472\tTraining_dev loss per batch: 0.002191\tTest_dev loss per batch: 0.000570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 5/25 [2:24:21<9:37:24, 1732.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 5 \tTraining loss per batch: 0.003979\tTraining_dev loss per batch: 0.002097\tTest_dev loss per batch: 0.000587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|‚ñà‚ñà‚ñç       | 6/25 [2:53:30<9:10:06, 1737.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 6 \tTraining loss per batch: 0.003242\tTraining_dev loss per batch: 0.002919\tTest_dev loss per batch: 0.001896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|‚ñà‚ñà‚ñä       | 7/25 [3:22:24<8:40:53, 1736.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 7 \tTraining loss per batch: 0.002900\tTraining_dev loss per batch: 0.002060\tTest_dev loss per batch: 0.001044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [3:51:17<8:11:43, 1735.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 8 \tTraining loss per batch: 0.003096\tTraining_dev loss per batch: 0.003948\tTest_dev loss per batch: 0.000991\n",
      "END OF EPOCH: 9 \tTraining loss per batch: 0.002763\tTraining_dev loss per batch: 0.001789\tTest_dev loss per batch: 0.000383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [4:49:10<7:14:00, 1736.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 10 \tTraining loss per batch: 0.002790\tTraining_dev loss per batch: 0.005005\tTest_dev loss per batch: 0.000578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [5:18:05<6:44:59, 1735.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 11 \tTraining loss per batch: 0.002682\tTraining_dev loss per batch: 0.002534\tTest_dev loss per batch: 0.000402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [5:46:47<6:15:07, 1731.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 12 \tTraining loss per batch: 0.002626\tTraining_dev loss per batch: 0.001829\tTest_dev loss per batch: 0.000428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [6:15:32<5:45:55, 1729.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 13 \tTraining loss per batch: 0.002508\tTraining_dev loss per batch: 0.001839\tTest_dev loss per batch: 0.000534\n",
      "END OF EPOCH: 14 \tTraining loss per batch: 0.002342\tTraining_dev loss per batch: 0.001714\tTest_dev loss per batch: 0.000447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [7:13:02<4:47:47, 1726.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 15 \tTraining loss per batch: 0.002244\tTraining_dev loss per batch: 0.001839\tTest_dev loss per batch: 0.000654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [7:42:01<4:19:35, 1730.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 16 \tTraining loss per batch: 0.002115\tTraining_dev loss per batch: 0.001997\tTest_dev loss per batch: 0.000576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [8:11:22<3:51:57, 1739.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 17 \tTraining loss per batch: 0.002131\tTraining_dev loss per batch: 0.001769\tTest_dev loss per batch: 0.000409\n",
      "END OF EPOCH: 18 \tTraining loss per batch: 0.002072\tTraining_dev loss per batch: 0.001722\tTest_dev loss per batch: 0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [9:11:27<2:57:39, 1776.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 19 \tTraining loss per batch: 0.001947\tTraining_dev loss per batch: 0.002005\tTest_dev loss per batch: 0.000592\n",
      "END OF EPOCH: 20 \tTraining loss per batch: 0.002088\tTraining_dev loss per batch: 0.001694\tTest_dev loss per batch: 0.000421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [10:13:58<2:01:48, 1827.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 21 \tTraining loss per batch: 0.002013\tTraining_dev loss per batch: 0.001991\tTest_dev loss per batch: 0.000450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [10:44:50<1:31:43, 1834.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 22 \tTraining loss per batch: 0.001892\tTraining_dev loss per batch: 0.002136\tTest_dev loss per batch: 0.000537\n",
      "END OF EPOCH: 23 \tTraining loss per batch: 0.001843\tTraining_dev loss per batch: 0.001608\tTest_dev loss per batch: 0.000363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [11:49:32<31:32, 1892.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 24 \tTraining loss per batch: 0.001864\tTraining_dev loss per batch: 0.002856\tTest_dev loss per batch: 0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [12:19:48<00:00, 1775.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 25 \tTraining loss per batch: 0.001782\tTraining_dev loss per batch: 0.003122\tTest_dev loss per batch: 0.000843\n",
      "[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [13:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7e116c20dc2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Train & save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Save losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-5128fdd7bce4>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(n_epochs, dataloaders, model, optimizer, criterion, device, save_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torchio/data/queue.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Patches list is empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0msample_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sampled_patches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torchio/data/queue.py\u001b[0m in \u001b[0;36m_fill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_subjects_for_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_per_volume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torchio/data/queue.py\u001b[0m in \u001b[0;36m_get_next_subject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# A StopIteration exception is expected when the queue is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Queue is empty:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/optic_chiasm/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model_parameters=[[64,1],[32,1],[16,2],[16,1],[8,2],[8,1],[4,4],[4,2],[4,1],[2,4],[2,2],[2,1],[1,8],[1,4],[1,2],[1,1]]\n",
    "model_parameters=[[2,4],[16,1],[4,2],[2,2],[8,2],[4,4],[64,1],[1,8]]\n",
    "\n",
    "folder='../../1_Data/2_Trained_AE/'\n",
    "\n",
    "for parameters in model_parameters:\n",
    "    \n",
    "    print(parameters)\n",
    "        \n",
    "    # Initialize the proper model\n",
    "    unet = UNet(1,1,parameters[0],parameters[1])\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(params=unet.parameters(), lr=0.005)\n",
    "    \n",
    "    # Create output folder\n",
    "    data_folder = folder+'/'+str(parameters[0])+'_'+str(parameters[1])+'/'\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    # Train & save weights\n",
    "    train_loss, dev_train_loss, dev_test_loss = train_network(n_epochs, dataloader, unet, optimizer, criterion, device, data_folder)\n",
    "    \n",
    "    # Save losses\n",
    "    with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_train_loss, f)\n",
    "        \n",
    "    with open(data_folder+'dev_test_loss.pkl', 'wb') as f:\n",
    "        pickle.dump(dev_test_loss, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
