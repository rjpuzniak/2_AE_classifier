{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torchio as tio\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchio as tio\n",
    "from torchio.transforms import (RescaleIntensity,RandomFlip,Compose, HistogramStandardization, CropOrPad, ToCanonical)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions_classification_training import UNet_1_layer, UNet_2_layer, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../subjects_dict.pkl', 'rb') as f:\n",
    "    subjects_dict = pickle.load(f)\n",
    "    \n",
    "# Remove CHP1 and ACH1 from dictionary\n",
    "subjects_dict['CHIASM']['control'].remove('CHP1')\n",
    "subjects_dict['CHIASM']['control'].remove('ACH1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for splitting the list\n",
    "def splitter(list_to_be_splitted, number_of_groups):\n",
    "    a, b = divmod(len(list_to_be_splitted), number_of_groups)\n",
    "    return (list_to_be_splitted[i*a+min(i,b):(i+1)*a+min(i+1,b)] for i in range(number_of_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning trained model\n",
    "def train_network(n_epochs, dataloaders, model, optimizer, criterion, device, save_path):\n",
    "    \n",
    "    track_train_loss = []\n",
    "    track_dev_train_loss = []\n",
    "    track_test_loss = []\n",
    "    \n",
    "    track_train_f1 = []\n",
    "    track_dev_train_f1 = []\n",
    "    track_test_f1 = []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        # Initialize loss monitoring variables\n",
    "        train_loss = 0.0\n",
    "        dev_train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "                \n",
    "        # Training\n",
    "        model.train()\n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['train']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output=model(data)\n",
    "            \n",
    "            loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss+= (loss.item()*len(batch['label']))\n",
    "            \n",
    "            acc_targets+=batch['label'][:].numpy().tolist()\n",
    "            acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "            \n",
    "        track_train_loss.append(train_loss/len(dict_kfold_combined_training['train']))        \n",
    "        track_train_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "            \n",
    "        # Validation on dev_train dataset\n",
    "        model.eval()\n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['dev_train']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "                \n",
    "                dev_train_loss+= (loss.item()*len(batch['label']))\n",
    "                \n",
    "                acc_targets+=batch['label'][:].numpy().tolist()\n",
    "                acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "                \n",
    "        track_dev_train_loss.append(dev_train_loss/len(dict_kfold_combined_training['dev_train']))\n",
    "        track_dev_train_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['test1']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "                \n",
    "                test_loss+= (loss.item()*len(batch['label']))\n",
    "                \n",
    "                acc_targets+=batch['label'][:].numpy().tolist()\n",
    "                acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "                \n",
    "        track_test_loss.append(test_loss/len(dict_kfold_combined_training['test1']))\n",
    "        track_test_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "        \n",
    "        if epoch%500 ==0:\n",
    "            print('END OF EPOCH: {} \\n Training loss per image: {:.6f}\\n Training_dev loss per image: {:.6f}\\n Test_dev loss per image: {:.6f}'.format(epoch, train_loss/len(dict_kfold_combined_training['train']),dev_train_loss/len(dict_kfold_combined_training['dev_train']),test_loss/len(dict_kfold_combined_training['test1'])))\n",
    "            \n",
    "        ## Save the model if reached min validation loss and save the number of epoch               \n",
    "        if dev_train_loss < valid_loss_min:\n",
    "            valid_loss_min = dev_train_loss\n",
    "            torch.save(model.state_dict(),save_path+'optimal_weights')\n",
    "            last_updated_epoch = epoch\n",
    "        \n",
    "            with open(save_path+'number_epochs.txt','w') as f:\n",
    "                print('Epoch:', str(epoch), file=f)  \n",
    "                \n",
    "        # Early stopping\n",
    "        if (epoch - last_updated_epoch) == 1000:\n",
    "            break\n",
    "                                \n",
    "    # return trained model\n",
    "    return track_train_loss, track_dev_train_loss, track_test_loss, track_train_f1, track_dev_train_f1, track_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor dataset in subjects_dict.keys():\\n    for label in subjects_dict[dataset].keys():\\n        if(dataset=='CHIASM' and label=='albinism'):\\n            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],9))\\n        else:\\n            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],8))\\n            \\n# Save the dictionary\\nwith open('design_kfold.pkl','wb') as f:\\n    pickle.dump(subjects_dict,f)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with splits\n",
    "'''\n",
    "for dataset in subjects_dict.keys():\n",
    "    for label in subjects_dict[dataset].keys():\n",
    "        if(dataset=='CHIASM' and label=='albinism'):\n",
    "            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],9))\n",
    "        else:\n",
    "            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],8))\n",
    "            \n",
    "# Save the dictionary\n",
    "with open('design_kfold.pkl','wb') as f:\n",
    "    pickle.dump(subjects_dict,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1740/1740 [00:27<00:00, 62.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Histogram standardization (to mitigate cross-site differences) - shared by all datasets\n",
    "chiasm_paths=[]\n",
    "\n",
    "# Obtain paths of all chiasm images\n",
    "for dataset in subjects_dict.keys():\n",
    "    for label in subjects_dict[dataset].keys():\n",
    "        for subject in subjects_dict[dataset][label]:\n",
    "            chiasm_paths.append('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz')\n",
    "\n",
    "chiasm_landmarks_path = Path('chiasm_landmarks.npy')    \n",
    "\n",
    "chiasm_landmarks = HistogramStandardization.train(chiasm_paths)\n",
    "torch.save(chiasm_landmarks, chiasm_landmarks_path)\n",
    "\n",
    "landmarks={'chiasm': chiasm_landmarks}\n",
    "\n",
    "standardize = HistogramStandardization(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation - shared by all datasets\n",
    "\n",
    "# Canonical\n",
    "canonical = ToCanonical()\n",
    "\n",
    "# Rescale\n",
    "rescale = RescaleIntensity((0,1))\n",
    "\n",
    "# Flip\n",
    "flip = RandomFlip((0,1,2), flip_probability=0.5, p=0.5)\n",
    "\n",
    "# Affine transformations\n",
    "affine = tio.RandomAffine(degrees=5, translation=(2,2,2), center='image')\n",
    "\n",
    "crop = CropOrPad((24,24,8))\n",
    "\n",
    "# Elastic deformation\n",
    "#elastic = tio.transforms.RandomElasticDeformation(num_control_points=4, max_displacement=4, locked_borders=1)\n",
    "\n",
    "# Composing transforms - flip serves as data augmentation and is used only for training\n",
    "transform_train = Compose([canonical, standardize, rescale, affine, flip, crop])\n",
    "transform_dev = Compose([canonical, standardize, rescale, crop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 45\n",
      "test2 Athletes control 6\n",
      "test2 HCP control 134\n",
      "test2 COBRE control 8\n",
      "test2 Leipzig control 17\n",
      "test2 MCIC control 4\n",
      "test2 214 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 3\n",
      "test1 UoN albinism 3\n",
      "test1 4 4\n",
      "dev_train ABIDE control 45\n",
      "dev_train Athletes control 6\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 8\n",
      "dev_train Leipzig control 17\n",
      "dev_train UoN control 3\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 216 4\n",
      "train ABIDE control 265\n",
      "train Athletes control 30\n",
      "train HCP control 798\n",
      "train COBRE control 44\n",
      "train Leipzig control 99\n",
      "train UoN control 14\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 18\n",
      "train 1274 23\n",
      "\n",
      "\n",
      "1\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 45\n",
      "test2 Athletes control 6\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 8\n",
      "test2 Leipzig control 17\n",
      "test2 MCIC control 3\n",
      "test2 212 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 3\n",
      "test1 UoN albinism 3\n",
      "test1 4 4\n",
      "dev_train ABIDE control 45\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 8\n",
      "dev_train Leipzig control 17\n",
      "dev_train UoN control 3\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 215 4\n",
      "train ABIDE control 265\n",
      "train Athletes control 31\n",
      "train HCP control 799\n",
      "train COBRE control 44\n",
      "train Leipzig control 99\n",
      "train UoN control 14\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1277 23\n",
      "\n",
      "\n",
      "2\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 45\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 8\n",
      "test2 Leipzig control 17\n",
      "test2 MCIC control 3\n",
      "test2 211 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 3\n",
      "test1 UoN albinism 3\n",
      "test1 4 4\n",
      "dev_train ABIDE control 44\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 8\n",
      "dev_train Leipzig control 17\n",
      "dev_train UoN control 3\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 214 4\n",
      "train ABIDE control 266\n",
      "train Athletes control 32\n",
      "train HCP control 799\n",
      "train COBRE control 44\n",
      "train Leipzig control 99\n",
      "train UoN control 14\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1279 23\n",
      "\n",
      "\n",
      "3\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 44\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 8\n",
      "test2 Leipzig control 17\n",
      "test2 MCIC control 3\n",
      "test2 210 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 3\n",
      "test1 UoN albinism 3\n",
      "test1 4 4\n",
      "dev_train ABIDE control 44\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 7\n",
      "dev_train Leipzig control 17\n",
      "dev_train UoN control 2\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 212 4\n",
      "train ABIDE control 267\n",
      "train Athletes control 32\n",
      "train HCP control 799\n",
      "train COBRE control 45\n",
      "train Leipzig control 99\n",
      "train UoN control 15\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1282 23\n",
      "\n",
      "\n",
      "4\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 44\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 7\n",
      "test2 Leipzig control 17\n",
      "test2 MCIC control 3\n",
      "test2 209 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 2\n",
      "test1 UoN albinism 3\n",
      "test1 3 4\n",
      "dev_train ABIDE control 44\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 7\n",
      "dev_train Leipzig control 16\n",
      "dev_train UoN control 2\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 211 4\n",
      "train ABIDE control 267\n",
      "train Athletes control 32\n",
      "train HCP control 799\n",
      "train COBRE control 46\n",
      "train Leipzig control 100\n",
      "train UoN control 16\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1285 23\n",
      "\n",
      "\n",
      "5\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 44\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 7\n",
      "test2 Leipzig control 16\n",
      "test2 MCIC control 3\n",
      "test2 208 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 2\n",
      "test1 UoN albinism 3\n",
      "test1 3 4\n",
      "dev_train ABIDE control 44\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 7\n",
      "dev_train Leipzig control 16\n",
      "dev_train UoN control 2\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 211 4\n",
      "train ABIDE control 267\n",
      "train Athletes control 32\n",
      "train HCP control 799\n",
      "train COBRE control 46\n",
      "train Leipzig control 101\n",
      "train UoN control 16\n",
      "train UoN albinism 17\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1286 23\n",
      "\n",
      "\n",
      "6\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 44\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 7\n",
      "test2 Leipzig control 16\n",
      "test2 MCIC control 3\n",
      "test2 208 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 2\n",
      "test1 UoN albinism 3\n",
      "test1 3 4\n",
      "dev_train ABIDE control 44\n",
      "dev_train Athletes control 5\n",
      "dev_train HCP control 133\n",
      "dev_train COBRE control 7\n",
      "dev_train Leipzig control 16\n",
      "dev_train UoN control 2\n",
      "dev_train UoN albinism 2\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 3\n",
      "dev_train 211 3\n",
      "train ABIDE control 267\n",
      "train Athletes control 32\n",
      "train HCP control 799\n",
      "train COBRE control 46\n",
      "train Leipzig control 101\n",
      "train UoN control 16\n",
      "train UoN albinism 18\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 19\n",
      "train 1286 24\n",
      "\n",
      "\n",
      "7\n",
      "test2 CHIASM albinism 1\n",
      "test2 CHIASM control 0\n",
      "test2 ABIDE control 44\n",
      "test2 Athletes control 5\n",
      "test2 HCP control 133\n",
      "test2 COBRE control 7\n",
      "test2 Leipzig control 16\n",
      "test2 MCIC control 3\n",
      "test2 208 1\n",
      "test1 CHIASM control 1\n",
      "test1 CHIASM albinism 1\n",
      "test1 UoN control 2\n",
      "test1 UoN albinism 2\n",
      "test1 3 3\n",
      "dev_train ABIDE control 45\n",
      "dev_train Athletes control 6\n",
      "dev_train HCP control 134\n",
      "dev_train COBRE control 8\n",
      "dev_train Leipzig control 17\n",
      "dev_train UoN control 3\n",
      "dev_train UoN albinism 3\n",
      "dev_train CHIASM control 1\n",
      "dev_train CHIASM albinism 1\n",
      "dev_train MCIC control 4\n",
      "dev_train 218 4\n",
      "train ABIDE control 266\n",
      "train Athletes control 31\n",
      "train HCP control 798\n",
      "train COBRE control 45\n",
      "train Leipzig control 100\n",
      "train UoN control 15\n",
      "train UoN albinism 18\n",
      "train CHIASM control 6\n",
      "train CHIASM albinism 6\n",
      "train MCIC control 18\n",
      "train 1279 24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the participants into 8 equal groups\n",
    "\n",
    "#              train test1 test2\n",
    "# control   80 - 10 - 0 - 10\n",
    "# albinism  80 - 10 - 10 -0\n",
    "\n",
    "groups=['train','dev_train','test1','test2']\n",
    "\n",
    "if not os.path.exists('../../1_Data/4_K-fold_combined'):\n",
    "    os.makedirs('../../1_Data/4_K-fold_combined')\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    output_folder='../../1_Data/4_K-fold_combined'+'/'+str(i)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the dictionary\n",
    "    with open('design_kfold.pkl','rb') as f:\n",
    "        kfold_design = pickle.load(f)\n",
    "\n",
    "    design_kfold_combined={}\n",
    "\n",
    "    # test2 - (i+1)-th group from CHIASM albinism + i-th group from all control groups\n",
    "\n",
    "    design_kfold_combined['test2']={}\n",
    "\n",
    "    # CHIASM albinism\n",
    "    design_kfold_combined['test2']['CHIASM']={}\n",
    "    design_kfold_combined['test2']['CHIASM']['albinism']=kfold_design['CHIASM']['albinism'][i]\n",
    "    design_kfold_combined['test2']['CHIASM']['control']=[]\n",
    "    kfold_design['CHIASM']['albinism'].pop(i)\n",
    "\n",
    "    # Other publicly available datasets of controls\n",
    "    for dataset in ['ABIDE', 'Athletes', 'HCP', 'COBRE', 'Leipzig', 'MCIC']:\n",
    "\n",
    "        design_kfold_combined['test2'][dataset]={}\n",
    "        design_kfold_combined['test2'][dataset]['control']=kfold_design[dataset]['control'][i]\n",
    "        kfold_design[dataset]['control'].pop(i)\n",
    "\n",
    "\n",
    "    # test 1 - (i or i+1)-th group per controls and albinism from CHIASM and UoN datasets\n",
    "\n",
    "    design_kfold_combined['test1']={}\n",
    "\n",
    "    for dataset in ['CHIASM','UoN']:\n",
    "        design_kfold_combined['test1'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            design_kfold_combined['test1'][dataset][label]=kfold_design[dataset][label][i]\n",
    "            kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # dev_train - (i+1)-th group\n",
    "\n",
    "    design_kfold_combined['dev_train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['dev_train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            if i==7:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][0]\n",
    "                kfold_design[dataset][label].pop(0)\n",
    "            else:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][i]\n",
    "                kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # train - rest\n",
    "\n",
    "    design_kfold_combined['train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():            \n",
    "            design_kfold_combined['train'][dataset][label]=[item for sublist in kfold_design[dataset][label] for item in sublist]\n",
    "\n",
    "    # Save the design\n",
    "    with open(output_folder+'/kfold_design_'+str(i)+'.pkl','wb') as f:\n",
    "        pickle.dump(design_kfold_combined, f)\n",
    "\n",
    "    # Sanity check by counting total number of entries\n",
    "    #total=0\n",
    "    #for k in design_kfold_combined.keys():\n",
    "    #    for l in design_kfold_combined[k].keys():\n",
    "    #        for m in design_kfold_combined[k][l].keys():\n",
    "    #            #print(k,l,m,len(design_kfold_combined[k][l][m]))\n",
    "    #            total+=len(design_kfold_combined[k][l][m])\n",
    "    #print(total, design_kfold_combined['test1']['UoN']['albinism'])\n",
    "\n",
    "\n",
    "    # Torchio's subjects' dictionary + upsample the albinism group, so it matches controls in train and dev_train + add labels\n",
    "\n",
    "    print(i)\n",
    "    for group in design_kfold_combined.keys():\n",
    "        total_con=0\n",
    "        total_alb=0\n",
    "        for dataset in design_kfold_combined[group].keys():\n",
    "            for label in design_kfold_combined[group][dataset].keys():\n",
    "                if label == 'control':\n",
    "                    total_con += len(design_kfold_combined[group][dataset][label])\n",
    "                else:\n",
    "                    total_alb += len(design_kfold_combined[group][dataset][label])\n",
    "                print(group,dataset,label, len(design_kfold_combined[group][dataset][label]) )\n",
    "        print(group, total_con, total_alb)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:22<5:36:10,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.018872\n",
      " Training_dev loss per image: 0.044259\n",
      " Test_dev loss per image: 1.424305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:40<5:11:17,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.005841\n",
      " Training_dev loss per image: 0.268762\n",
      " Test_dev loss per image: 1.487818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [1:06:56<4:48:25,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.002991\n",
      " Training_dev loss per image: 0.776391\n",
      " Test_dev loss per image: 1.306464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1549/8000 [1:09:10<4:48:06,  2.68s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:22<5:36:56,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.012344\n",
      " Training_dev loss per image: 0.099344\n",
      " Test_dev loss per image: 2.287208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:44<5:13:57,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.011277\n",
      " Training_dev loss per image: 0.105287\n",
      " Test_dev loss per image: 2.215648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [1:07:07<4:50:35,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.007425\n",
      " Training_dev loss per image: 0.340033\n",
      " Test_dev loss per image: 2.546013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1845/8000 [1:22:35<4:35:32,  2.69s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:19<5:34:28,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.015172\n",
      " Training_dev loss per image: 0.816794\n",
      " Test_dev loss per image: 1.198089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:41<5:12:19,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.007595\n",
      " Training_dev loss per image: 1.057677\n",
      " Test_dev loss per image: 1.040145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1005/8000 [44:57<5:12:57,  2.68s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:21<5:34:39,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.015467\n",
      " Training_dev loss per image: 0.060385\n",
      " Test_dev loss per image: 1.080209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:44<5:13:38,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.005846\n",
      " Training_dev loss per image: 0.015214\n",
      " Test_dev loss per image: 0.986655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [1:07:08<4:50:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.002199\n",
      " Training_dev loss per image: 0.017223\n",
      " Test_dev loss per image: 1.598958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2000/8000 [1:29:31<4:28:36,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.005665\n",
      " Training_dev loss per image: 0.023957\n",
      " Test_dev loss per image: 1.271301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2500/8000 [1:51:54<4:05:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2500 \n",
      " Training loss per image: 0.003324\n",
      " Training_dev loss per image: 0.023814\n",
      " Test_dev loss per image: 1.546930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 2793/8000 [2:05:04<3:53:10,  2.69s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:22<5:33:45,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.015506\n",
      " Training_dev loss per image: 2.007509\n",
      " Test_dev loss per image: 0.311409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:45<5:14:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.004244\n",
      " Training_dev loss per image: 2.009826\n",
      " Test_dev loss per image: 0.157496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1029/8000 [46:05<5:12:15,  2.69s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:23<5:36:19,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.009795\n",
      " Training_dev loss per image: 0.507828\n",
      " Test_dev loss per image: 1.706899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:45<5:15:36,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.006353\n",
      " Training_dev loss per image: 0.552350\n",
      " Test_dev loss per image: 2.304218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1100/8000 [49:16<5:09:06,  2.69s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:26<5:39:20,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.012126\n",
      " Training_dev loss per image: 0.013986\n",
      " Test_dev loss per image: 0.038230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:53<5:14:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.007217\n",
      " Training_dev loss per image: 0.003035\n",
      " Test_dev loss per image: 0.019696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [1:07:17<4:48:52,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.005258\n",
      " Training_dev loss per image: 0.002416\n",
      " Test_dev loss per image: 0.125806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2000/8000 [1:29:41<4:27:16,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.005156\n",
      " Training_dev loss per image: 0.014338\n",
      " Test_dev loss per image: 0.018678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2500/8000 [1:52:06<4:06:43,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2500 \n",
      " Training loss per image: 0.001617\n",
      " Training_dev loss per image: 0.018667\n",
      " Test_dev loss per image: 0.007828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2565/8000 [1:55:04<4:03:49,  2.69s/it]\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [22:27<5:36:26,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.022738\n",
      " Training_dev loss per image: 0.150625\n",
      " Test_dev loss per image: 0.025566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [44:54<5:13:22,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.006245\n",
      " Training_dev loss per image: 0.171699\n",
      " Test_dev loss per image: 0.569550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [1:07:22<4:52:15,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.009556\n",
      " Training_dev loss per image: 0.355289\n",
      " Test_dev loss per image: 0.066921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2000/8000 [1:29:49<4:28:37,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.003083\n",
      " Training_dev loss per image: 0.042182\n",
      " Test_dev loss per image: 0.013214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2480/8000 [1:51:25<4:08:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the participants into 8 equal groups\n",
    "\n",
    "#              train test1 test2\n",
    "# control   80 - 10 - 0 - 10\n",
    "# albinism  80 - 10 - 10 -0\n",
    "\n",
    "groups=['train','dev_train','test1','test2']\n",
    "\n",
    "if not os.path.exists('../../1_Data/4_K-fold_combined'):\n",
    "    os.makedirs('../../1_Data/4_K-fold_combined')\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    output_folder='../../1_Data/4_K-fold_combined'+'/'+str(i)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the dictionary\n",
    "    with open('design_kfold.pkl','rb') as f:\n",
    "        kfold_design = pickle.load(f)\n",
    "\n",
    "    design_kfold_combined={}\n",
    "\n",
    "    # test2 - (i+1)-th group from CHIASM albinism + i-th group from all control groups\n",
    "\n",
    "    design_kfold_combined['test2']={}\n",
    "\n",
    "    # CHIASM albinism\n",
    "    design_kfold_combined['test2']['CHIASM']={}\n",
    "    design_kfold_combined['test2']['CHIASM']['albinism']=kfold_design['CHIASM']['albinism'][i]\n",
    "    design_kfold_combined['test2']['CHIASM']['control']=[]\n",
    "    kfold_design['CHIASM']['albinism'].pop(i)\n",
    "\n",
    "    # Other publicly available datasets of controls\n",
    "    for dataset in ['ABIDE', 'Athletes', 'HCP', 'COBRE', 'Leipzig', 'MCIC']:\n",
    "\n",
    "        design_kfold_combined['test2'][dataset]={}\n",
    "        design_kfold_combined['test2'][dataset]['control']=kfold_design[dataset]['control'][i]\n",
    "        kfold_design[dataset]['control'].pop(i)\n",
    "\n",
    "\n",
    "    # test 1 - (i or i+1)-th group per controls and albinism from CHIASM and UoN datasets\n",
    "\n",
    "    design_kfold_combined['test1']={}\n",
    "\n",
    "    for dataset in ['CHIASM','UoN']:\n",
    "        design_kfold_combined['test1'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            design_kfold_combined['test1'][dataset][label]=kfold_design[dataset][label][i]\n",
    "            kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # dev_train - (i+1)-th group\n",
    "\n",
    "    design_kfold_combined['dev_train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['dev_train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            if i==7:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][0]\n",
    "                kfold_design[dataset][label].pop(0)\n",
    "            else:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][i]\n",
    "                kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # train - rest\n",
    "\n",
    "    design_kfold_combined['train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():            \n",
    "            design_kfold_combined['train'][dataset][label]=[item for sublist in kfold_design[dataset][label] for item in sublist]\n",
    "\n",
    "    # Save the design\n",
    "    with open(output_folder+'/kfold_design_'+str(i)+'.pkl','wb') as f:\n",
    "        pickle.dump(design_kfold_combined, f)\n",
    "\n",
    "    # Sanity check by counting total number of entries\n",
    "    #total=0\n",
    "    #for k in design_kfold_combined.keys():\n",
    "    #    for l in design_kfold_combined[k].keys():\n",
    "    #        for m in design_kfold_combined[k][l].keys():\n",
    "    #            #print(k,l,m,len(design_kfold_combined[k][l][m]))\n",
    "    #            total+=len(design_kfold_combined[k][l][m])\n",
    "    #print(total, design_kfold_combined['test1']['UoN']['albinism'])\n",
    "\n",
    "\n",
    "    # Torchio's subjects' dictionary + upsample the albinism group, so it matches controls in train and dev_train + add labels\n",
    "\n",
    "    print(i)\n",
    "    #for group in design_kfold_combined.keys():\n",
    "    #    total_con=0\n",
    "    #    total_alb=0\n",
    "    #    for dataset in design_kfold_combined[group].keys():\n",
    "    #        for label in design_kfold_combined[group][dataset].keys():\n",
    "    #            if label == 'control':\n",
    "    #                total_con += len(design_kfold_combined[group][dataset][label])\n",
    "    #            else:\n",
    "    #                total_alb += len(design_kfold_combined[group][dataset][label])\n",
    "    #            #print(group,dataset,label, len(design_kfold_combined[group][dataset][label]) )\n",
    "    #    print(group, total_con, total_alb)\n",
    "    #print('\\n')\n",
    "    \n",
    "    dict_kfold_combined_training={}\n",
    "\n",
    "    for group in design_kfold_combined.keys():\n",
    "\n",
    "        dict_kfold_combined_training[group]=[]\n",
    "\n",
    "        # Calculate the number of albinism and controls, calculate the scaling coefficient\n",
    "        num_control=0\n",
    "        num_albinism=0\n",
    "\n",
    "        for dataset in design_kfold_combined[group].keys():\n",
    "\n",
    "            num_control+=len(design_kfold_combined[group][dataset]['control'])\n",
    "\n",
    "            if dataset in ['CHIASM', 'UoN']:\n",
    "                num_albinism+=len(design_kfold_combined[group][dataset]['albinism'])\n",
    "\n",
    "        scaling_factor=int(num_control/num_albinism)\n",
    "\n",
    "        # Create Torchio's subject for listed IDs, for train & dev_train upsample the albinism\n",
    "        for dataset in design_kfold_combined[group].keys():\n",
    "\n",
    "            # If test just aggregate all the data\n",
    "            if (group=='test2' or group == 'test1'):\n",
    "\n",
    "                for label in design_kfold_combined[group][dataset].keys():\n",
    "\n",
    "                    if label=='albinism':\n",
    "                        label_as=1\n",
    "                    elif label=='control':\n",
    "                        label_as=0\n",
    "\n",
    "                    dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                        label=label_as) for subject in design_kfold_combined[group][dataset][label]]\n",
    "\n",
    "            # otherwise upsample albinism by calculated scaling_factor\n",
    "            else:\n",
    "\n",
    "                for label in design_kfold_combined[group][dataset].keys():\n",
    "\n",
    "                    if label=='control':\n",
    "\n",
    "                        label_as=0\n",
    "\n",
    "                        dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                        label=label_as) for subject in design_kfold_combined[group][dataset][label]]\n",
    "\n",
    "                    if label=='albinism':\n",
    "\n",
    "                        label_as=1\n",
    "\n",
    "                        for i in range(scaling_factor):\n",
    "\n",
    "                            dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                              label=label_as) for subject in design_kfold_combined[group][dataset][label]] \n",
    "\n",
    "                            \n",
    "    #for group in dict_kfold_combined_training.keys():\n",
    "    #    print(len(dict_kfold_combined_training[group]))\n",
    "    #print('\\n')\n",
    "    \n",
    "    \n",
    "    datasets_list={}\n",
    "\n",
    "    for group in dict_kfold_combined_training.keys():\n",
    "\n",
    "        if group =='train':\n",
    "\n",
    "            datasets_list[group] = tio.SubjectsDataset(dict_kfold_combined_training[group], transform=transform_train)\n",
    "\n",
    "        else:\n",
    "\n",
    "            datasets_list[group] = tio.SubjectsDataset(dict_kfold_combined_training[group], transform=transform_dev)\n",
    "\n",
    "\n",
    "    # Create dataloaders\n",
    "    dataloaders_chiasm={'train': DataLoader(dataset=datasets_list['train'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'dev_train': DataLoader(dataset=datasets_list['dev_train'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'test1': DataLoader(dataset=datasets_list['test1'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'test2': DataLoader(dataset=datasets_list['test2'], batch_size=10, shuffle=True, num_workers=8)}\n",
    "\n",
    "    # Try setting CUDA if possible\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\") \n",
    "\n",
    "    # Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model_parameters=[[1,2,2,1,256]]\n",
    "    learning_rates = [0.00005]\n",
    "    n_epochs=8000\n",
    "\n",
    "    folder=output_folder\n",
    "\n",
    "    for parameters in model_parameters:\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            # Initialize the proper model\n",
    "            classifying_network = Classifier(parameters[0],parameters[1], parameters[2], parameters[3], parameters[4])\n",
    "            classifying_network.freeze_feature_extraction()\n",
    "\n",
    "            # Optimizer    \n",
    "            optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, classifying_network.parameters()), lr=learning_rate)\n",
    "            #optimizer = torch.optim.Adam(params=classifying_network.parameters(), lr=0.00005)\n",
    "\n",
    "            # Create output folder\n",
    "            data_folder = folder+'/'+str(parameters[0])+'_'+str(parameters[1])+'_'+str(parameters[2])+'_'+str(parameters[3])+'_'+str(parameters[4])+'_'+str(learning_rate)+'/'\n",
    "            os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "            # Train & save weights\n",
    "            train_loss, dev_train_loss, test_loss, train_f1, dev_train_f1, test_f1 = train_network(n_epochs, dataloaders_chiasm, classifying_network, optimizer, criterion, device, data_folder)\n",
    "\n",
    "            # Save losses\n",
    "            with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(train_loss, f)\n",
    "\n",
    "            with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(dev_train_loss, f)\n",
    "\n",
    "            with open(data_folder+'test_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(test_loss, f)\n",
    "\n",
    "            with open(data_folder+'train_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(train_f1, f)\n",
    "\n",
    "            with open(data_folder+'dev_train_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(dev_train_f1, f)\n",
    "\n",
    "            with open(data_folder+'test_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(test_f1, f)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../1_Data/4_K-fold_combined/5/kfold_design_5.pkl', 'rb') as f:\n",
    "    test= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in test.keys():\n",
    "    #print(group)\n",
    "    for dataset in test[group].keys():\n",
    "        #print(dataset)\n",
    "        for label in test[group][dataset].keys():\n",
    "            print(group,dataset,label,len(test[group][dataset][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
