{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torchio as tio\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchio as tio\n",
    "from torchio.transforms import (RescaleIntensity,RandomFlip,Compose, HistogramStandardization, CropOrPad, ToCanonical)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Functions_classification_training import UNet_1_layer, UNet_2_layer, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../subjects_dict.pkl', 'rb') as f:\n",
    "    subjects_dict = pickle.load(f)\n",
    "    \n",
    "# Remove CHP1 and ACH1 from dictionary\n",
    "subjects_dict['CHIASM']['control'].remove('CHP1')\n",
    "subjects_dict['CHIASM']['control'].remove('ACH1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for splitting the list\n",
    "def splitter(list_to_be_splitted, number_of_groups):\n",
    "    a, b = divmod(len(list_to_be_splitted), number_of_groups)\n",
    "    return (list_to_be_splitted[i*a+min(i,b):(i+1)*a+min(i+1,b)] for i in range(number_of_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning trained model\n",
    "def train_network(n_epochs, dataloaders, model, optimizer, criterion, device, save_path):\n",
    "    \n",
    "    track_train_loss = []\n",
    "    track_dev_train_loss = []\n",
    "    track_test_loss = []\n",
    "    \n",
    "    track_train_f1 = []\n",
    "    track_dev_train_f1 = []\n",
    "    track_test_f1 = []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        # Initialize loss monitoring variables\n",
    "        train_loss = 0.0\n",
    "        dev_train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "                \n",
    "        # Training\n",
    "        model.train()\n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['train']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output=model(data)\n",
    "            \n",
    "            loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss+= (loss.item()*len(batch['label']))\n",
    "            \n",
    "            acc_targets+=batch['label'][:].numpy().tolist()\n",
    "            acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "            \n",
    "        track_train_loss.append(train_loss/len(dict_kfold_combined_training['train']))        \n",
    "        track_train_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "            \n",
    "        # Validation on dev_train dataset\n",
    "        model.eval()\n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['dev_train']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "                \n",
    "                dev_train_loss+= (loss.item()*len(batch['label']))\n",
    "                \n",
    "                acc_targets+=batch['label'][:].numpy().tolist()\n",
    "                acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "                \n",
    "        track_dev_train_loss.append(dev_train_loss/len(dict_kfold_combined_training['dev_train']))\n",
    "        track_dev_train_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "        \n",
    "        acc_targets=[]\n",
    "        acc_predictions=[]\n",
    "        \n",
    "        for batch in dataloaders['test1']:\n",
    "            \n",
    "            data = batch['chiasm']['data'].to(device)\n",
    "            data.requires_grad = True\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output[:,0], batch['label'].to(device).float())\n",
    "                \n",
    "                test_loss+= (loss.item()*len(batch['label']))\n",
    "                \n",
    "                acc_targets+=batch['label'][:].numpy().tolist()\n",
    "                acc_predictions+=output.round().detach().cpu().numpy().tolist()\n",
    "                \n",
    "        track_test_loss.append(test_loss/len(dict_kfold_combined_training['test1']))\n",
    "        track_test_f1.append(f1_score(acc_targets, acc_predictions, average='weighted')) \n",
    "        \n",
    "        if epoch%500 ==0:\n",
    "            print('END OF EPOCH: {} \\n Training loss per image: {:.6f}\\n Training_dev loss per image: {:.6f}\\n Test_dev loss per image: {:.6f}'.format(epoch, train_loss/len(dict_kfold_combined_training['train']),dev_train_loss/len(dict_kfold_combined_training['dev_train']),test_loss/len(dict_kfold_combined_training['test1'])))\n",
    "            \n",
    "        ## Save the model if reached min validation loss and save the number of epoch               \n",
    "        if dev_train_loss < valid_loss_min:\n",
    "            valid_loss_min = dev_train_loss\n",
    "            torch.save(model.state_dict(),save_path+'optimal_weights')\n",
    "            last_updated_epoch = epoch\n",
    "        \n",
    "            with open(save_path+'number_epochs.txt','w') as f:\n",
    "                print('Epoch:', str(epoch), file=f)  \n",
    "                \n",
    "        # Early stopping\n",
    "        if (epoch - last_updated_epoch) == 1000:\n",
    "            break\n",
    "                                \n",
    "    # return trained model\n",
    "    return track_train_loss, track_dev_train_loss, track_test_loss, track_train_f1, track_dev_train_f1, track_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor dataset in subjects_dict.keys():\\n    for label in subjects_dict[dataset].keys():\\n        if(dataset=='CHIASM' and label=='albinism'):\\n            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],9))\\n        else:\\n            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],8))\\n            \\n# Save the dictionary\\nwith open('design_kfold.pkl','wb') as f:\\n    pickle.dump(subjects_dict,f)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with splits\n",
    "'''\n",
    "for dataset in subjects_dict.keys():\n",
    "    for label in subjects_dict[dataset].keys():\n",
    "        if(dataset=='CHIASM' and label=='albinism'):\n",
    "            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],9))\n",
    "        else:\n",
    "            subjects_dict[dataset][label]=list(splitter(subjects_dict[dataset][label],8))\n",
    "            \n",
    "# Save the dictionary\n",
    "with open('design_kfold.pkl','wb') as f:\n",
    "    pickle.dump(subjects_dict,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 1740/1740 [00:01<00:00, 1003.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Histogram standardization (to mitigate cross-site differences) - shared by all datasets\n",
    "chiasm_paths=[]\n",
    "\n",
    "# Obtain paths of all chiasm images\n",
    "for dataset in subjects_dict.keys():\n",
    "    for label in subjects_dict[dataset].keys():\n",
    "        for subject in subjects_dict[dataset][label]:\n",
    "            chiasm_paths.append('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz')\n",
    "\n",
    "chiasm_landmarks_path = Path('chiasm_landmarks.npy')    \n",
    "\n",
    "chiasm_landmarks = HistogramStandardization.train(chiasm_paths)\n",
    "torch.save(chiasm_landmarks, chiasm_landmarks_path)\n",
    "\n",
    "landmarks={'chiasm': chiasm_landmarks}\n",
    "\n",
    "standardize = HistogramStandardization(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation - shared by all datasets\n",
    "\n",
    "# Canonical\n",
    "canonical = ToCanonical()\n",
    "\n",
    "# Rescale\n",
    "rescale = RescaleIntensity((0,1))\n",
    "\n",
    "# Flip\n",
    "flip = RandomFlip((0,1,2), flip_probability=0.5, p=0.5)\n",
    "\n",
    "# Affine transformations\n",
    "affine = tio.RandomAffine(degrees=5, translation=(2,2,2), center='image')\n",
    "\n",
    "crop = CropOrPad((24,24,8))\n",
    "\n",
    "# Elastic deformation\n",
    "#elastic = tio.transforms.RandomElasticDeformation(num_control_points=4, max_displacement=4, locked_borders=1)\n",
    "\n",
    "# Composing transforms - flip serves as data augmentation and is used only for training\n",
    "transform_train = Compose([canonical, standardize, rescale, affine, flip, crop])\n",
    "transform_dev = Compose([canonical, standardize, rescale, crop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [26:37<6:38:59,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.011216\n",
      " Training_dev loss per image: 0.065965\n",
      " Test_dev loss per image: 1.751088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [52:45<5:55:58,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.011320\n",
      " Training_dev loss per image: 0.044103\n",
      " Test_dev loss per image: 1.160551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████▉                                                              | 1098/8000 [57:47<6:03:18,  3.16s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:33<6:21:55,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.002533\n",
      " Training_dev loss per image: 0.028369\n",
      " Test_dev loss per image: 2.116751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:05<5:59:56,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.002027\n",
      " Training_dev loss per image: 0.038738\n",
      " Test_dev loss per image: 3.067864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▏                                                        | 1500/8000 [1:16:37<5:31:38,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.004184\n",
      " Training_dev loss per image: 0.036440\n",
      " Test_dev loss per image: 3.039875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████▏                                                    | 1959/8000 [1:40:06<5:08:43,  3.07s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:32<6:23:52,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.045569\n",
      " Training_dev loss per image: 0.265317\n",
      " Test_dev loss per image: 2.471508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:04<5:57:38,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.037037\n",
      " Training_dev loss per image: 0.192158\n",
      " Test_dev loss per image: 2.615460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▏                                                        | 1500/8000 [1:16:39<5:31:58,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.033535\n",
      " Training_dev loss per image: 0.237462\n",
      " Test_dev loss per image: 2.783979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████▌                                                     | 1895/8000 [1:36:53<5:12:09,  3.07s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:35<6:23:59,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.002272\n",
      " Training_dev loss per image: 0.014593\n",
      " Test_dev loss per image: 0.922518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:11<5:56:14,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.002389\n",
      " Training_dev loss per image: 0.021338\n",
      " Test_dev loss per image: 0.784763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▏                                                        | 1500/8000 [1:16:46<5:32:24,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.001205\n",
      " Training_dev loss per image: 0.018415\n",
      " Test_dev loss per image: 1.074311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████▌                                                    | 2000/8000 [1:42:22<5:07:10,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.002099\n",
      " Training_dev loss per image: 0.024548\n",
      " Test_dev loss per image: 1.088626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████▉                                                | 2500/8000 [2:07:58<4:41:20,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2500 \n",
      " Training loss per image: 0.001193\n",
      " Training_dev loss per image: 0.021254\n",
      " Test_dev loss per image: 0.978087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████▉                                               | 2625/8000 [2:14:25<4:35:14,  3.07s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:32<6:21:18,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.032740\n",
      " Training_dev loss per image: 0.125357\n",
      " Test_dev loss per image: 2.646341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:04<5:55:28,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.027955\n",
      " Training_dev loss per image: 0.158279\n",
      " Test_dev loss per image: 2.841789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▏                                                        | 1500/8000 [1:16:36<5:32:19,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.028300\n",
      " Training_dev loss per image: 0.130969\n",
      " Test_dev loss per image: 2.874740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████▌                                                    | 2000/8000 [1:42:13<5:08:21,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.018851\n",
      " Training_dev loss per image: 0.099332\n",
      " Test_dev loss per image: 2.283489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████▊                                                  | 2264/8000 [1:55:50<4:53:30,  3.07s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:45<6:25:34,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.024899\n",
      " Training_dev loss per image: 0.095526\n",
      " Test_dev loss per image: 2.932807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:32<6:00:12,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.032119\n",
      " Training_dev loss per image: 0.039189\n",
      " Test_dev loss per image: 1.664609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▌                                                           | 1204/8000 [1:02:06<5:50:33,  3.10s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:48<6:27:14,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.005129\n",
      " Training_dev loss per image: 0.000283\n",
      " Test_dev loss per image: 1.387370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:37<6:00:22,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.009497\n",
      " Training_dev loss per image: 0.000971\n",
      " Test_dev loss per image: 1.013940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████▏                                                        | 1500/8000 [1:17:27<5:35:04,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1500 \n",
      " Training loss per image: 0.005320\n",
      " Training_dev loss per image: 0.001847\n",
      " Test_dev loss per image: 0.885683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████▌                                                    | 2000/8000 [1:43:16<5:08:55,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2000 \n",
      " Training loss per image: 0.001372\n",
      " Training_dev loss per image: 0.000124\n",
      " Test_dev loss per image: 0.641684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████▉                                                | 2500/8000 [2:09:10<4:47:28,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 2500 \n",
      " Training loss per image: 0.003129\n",
      " Training_dev loss per image: 0.000105\n",
      " Test_dev loss per image: 0.971308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████▋                                               | 2599/8000 [2:14:26<4:39:22,  3.10s/it]\n",
      "  0%|                                                                                     | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                    | 500/8000 [25:48<6:28:39,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 500 \n",
      " Training loss per image: 0.005498\n",
      " Training_dev loss per image: 0.064644\n",
      " Test_dev loss per image: 0.065955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                               | 1000/8000 [51:38<6:02:04,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH: 1000 \n",
      " Training loss per image: 0.005475\n",
      " Training_dev loss per image: 0.296456\n",
      " Test_dev loss per image: 0.042625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████                                                               | 1001/8000 [51:44<6:01:49,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the participants into 8 equal groups\n",
    "\n",
    "#              train test1 test2\n",
    "# control   80 - 10 - 0 - 10\n",
    "# albinism  80 - 10 - 10 -0\n",
    "\n",
    "groups=['train','dev_train','test1','test2']\n",
    "\n",
    "if not os.path.exists('../../1_Data/4_K-fold_combined_extraction_learning_rate'):\n",
    "    os.makedirs('../../1_Data/4_K-fold_combined_extraction_learning_rate')\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    iteration=i\n",
    "    \n",
    "    output_folder='../../1_Data/4_K-fold_combined_extraction_learning_rate'+'/'+str(i)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the dictionary\n",
    "    with open('design_kfold.pkl','rb') as f:\n",
    "        kfold_design = pickle.load(f)\n",
    "\n",
    "    design_kfold_combined={}\n",
    "\n",
    "    # test2 - (i+1)-th group from CHIASM albinism + i-th group from all control groups\n",
    "\n",
    "    design_kfold_combined['test2']={}\n",
    "\n",
    "    # CHIASM albinism\n",
    "    design_kfold_combined['test2']['CHIASM']={}\n",
    "    design_kfold_combined['test2']['CHIASM']['albinism']=kfold_design['CHIASM']['albinism'][i]\n",
    "    design_kfold_combined['test2']['CHIASM']['control']=[]\n",
    "    kfold_design['CHIASM']['albinism'].pop(i)\n",
    "\n",
    "    # Other publicly available datasets of controls\n",
    "    for dataset in ['ABIDE', 'Athletes', 'HCP', 'COBRE', 'Leipzig', 'MCIC']:\n",
    "\n",
    "        design_kfold_combined['test2'][dataset]={}\n",
    "        design_kfold_combined['test2'][dataset]['control']=kfold_design[dataset]['control'][i]\n",
    "        kfold_design[dataset]['control'].pop(i)\n",
    "\n",
    "\n",
    "    # test 1 - (i or i+1)-th group per controls and albinism from CHIASM and UoN datasets\n",
    "\n",
    "    design_kfold_combined['test1']={}\n",
    "\n",
    "    for dataset in ['CHIASM','UoN']:\n",
    "        design_kfold_combined['test1'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            design_kfold_combined['test1'][dataset][label]=kfold_design[dataset][label][i]\n",
    "            kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # dev_train - (i+1)-th group\n",
    "\n",
    "    design_kfold_combined['dev_train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['dev_train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():\n",
    "            if i==7:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][0]\n",
    "                kfold_design[dataset][label].pop(0)\n",
    "            else:\n",
    "                design_kfold_combined['dev_train'][dataset][label]=kfold_design[dataset][label][i]\n",
    "                kfold_design[dataset][label].pop(i)\n",
    "\n",
    "\n",
    "    # train - rest\n",
    "\n",
    "    design_kfold_combined['train']={}\n",
    "\n",
    "    for dataset in kfold_design.keys():\n",
    "        design_kfold_combined['train'][dataset]={}\n",
    "        for label in kfold_design[dataset].keys():            \n",
    "            design_kfold_combined['train'][dataset][label]=[item for sublist in kfold_design[dataset][label] for item in sublist]\n",
    "\n",
    "    # Save the design\n",
    "    with open(output_folder+'/kfold_design_'+str(i)+'.pkl','wb') as f:\n",
    "        pickle.dump(design_kfold_combined, f)\n",
    "\n",
    "    # Sanity check by counting total number of entries\n",
    "    #total=0\n",
    "    #for k in design_kfold_combined.keys():\n",
    "    #    for l in design_kfold_combined[k].keys():\n",
    "    #        for m in design_kfold_combined[k][l].keys():\n",
    "    #            #print(k,l,m,len(design_kfold_combined[k][l][m]))\n",
    "    #            total+=len(design_kfold_combined[k][l][m])\n",
    "    #print(total, design_kfold_combined['test1']['UoN']['albinism'])\n",
    "\n",
    "\n",
    "    # Torchio's subjects' dictionary + upsample the albinism group, so it matches controls in train and dev_train + add labels\n",
    "\n",
    "    print(i)\n",
    "    #for group in design_kfold_combined.keys():\n",
    "    #    total_con=0\n",
    "    #    total_alb=0\n",
    "    #    for dataset in design_kfold_combined[group].keys():\n",
    "    #        for label in design_kfold_combined[group][dataset].keys():\n",
    "    #            if label == 'control':\n",
    "    #                total_con += len(design_kfold_combined[group][dataset][label])\n",
    "    #            else:\n",
    "    #                total_alb += len(design_kfold_combined[group][dataset][label])\n",
    "    #            #print(group,dataset,label, len(design_kfold_combined[group][dataset][label]) )\n",
    "    #    print(group, total_con, total_alb)\n",
    "    #print('\\n')\n",
    "    \n",
    "    dict_kfold_combined_training={}\n",
    "\n",
    "    for group in design_kfold_combined.keys():\n",
    "\n",
    "        dict_kfold_combined_training[group]=[]\n",
    "\n",
    "        # Calculate the number of albinism and controls, calculate the scaling coefficient\n",
    "        num_control=0\n",
    "        num_albinism=0\n",
    "\n",
    "        for dataset in design_kfold_combined[group].keys():\n",
    "\n",
    "            num_control+=len(design_kfold_combined[group][dataset]['control'])\n",
    "\n",
    "            if dataset in ['CHIASM', 'UoN']:\n",
    "                num_albinism+=len(design_kfold_combined[group][dataset]['albinism'])\n",
    "\n",
    "        scaling_factor=int(num_control/num_albinism)\n",
    "\n",
    "        # Create Torchio's subject for listed IDs, for train & dev_train upsample the albinism\n",
    "        for dataset in design_kfold_combined[group].keys():\n",
    "\n",
    "            # If test just aggregate all the data\n",
    "            if (group=='test2' or group == 'test1'):\n",
    "\n",
    "                for label in design_kfold_combined[group][dataset].keys():\n",
    "\n",
    "                    if label=='albinism':\n",
    "                        label_as=1\n",
    "                    elif label=='control':\n",
    "                        label_as=0\n",
    "\n",
    "                    dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                        label=label_as) for subject in design_kfold_combined[group][dataset][label]]\n",
    "\n",
    "            # otherwise upsample albinism by calculated scaling_factor\n",
    "            else:\n",
    "\n",
    "                for label in design_kfold_combined[group][dataset].keys():\n",
    "\n",
    "                    if label=='control':\n",
    "\n",
    "                        label_as=0\n",
    "\n",
    "                        dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                        label=label_as) for subject in design_kfold_combined[group][dataset][label]]\n",
    "\n",
    "                    if label=='albinism':\n",
    "\n",
    "                        label_as=1\n",
    "\n",
    "                        for i in range(scaling_factor):\n",
    "\n",
    "                            dict_kfold_combined_training[group]+=[tio.Subject(chiasm=tio.Image('../../1_Data/1_Input/'+dataset+'/'+subject+'/chiasm.nii.gz', type=tio.INTENSITY),\n",
    "                                                                              label=label_as) for subject in design_kfold_combined[group][dataset][label]] \n",
    "\n",
    "                            \n",
    "    #for group in dict_kfold_combined_training.keys():\n",
    "    #    print(len(dict_kfold_combined_training[group]))\n",
    "    #print('\\n')\n",
    "    \n",
    "    \n",
    "    datasets_list={}\n",
    "\n",
    "    for group in dict_kfold_combined_training.keys():\n",
    "\n",
    "        if group =='train':\n",
    "\n",
    "            datasets_list[group] = tio.SubjectsDataset(dict_kfold_combined_training[group], transform=transform_train)\n",
    "\n",
    "        else:\n",
    "\n",
    "            datasets_list[group] = tio.SubjectsDataset(dict_kfold_combined_training[group], transform=transform_dev)\n",
    "\n",
    "\n",
    "    # Create dataloaders\n",
    "    dataloaders_chiasm={'train': DataLoader(dataset=datasets_list['train'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'dev_train': DataLoader(dataset=datasets_list['dev_train'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'test1': DataLoader(dataset=datasets_list['test1'], batch_size=10, shuffle=True, num_workers=8),\n",
    "                       'test2': DataLoader(dataset=datasets_list['test2'], batch_size=10, shuffle=True, num_workers=8)}\n",
    "\n",
    "    # Try setting CUDA if possible\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\") \n",
    "\n",
    "    # Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model_parameters=[[1,2,2,1,256]]\n",
    "    learning_rates = [0.0005]\n",
    "    n_epochs=8000\n",
    "\n",
    "    folder=output_folder\n",
    "\n",
    "    for parameters in model_parameters:\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            # Initialize the proper model\n",
    "            classifying_network = Classifier(parameters[0],parameters[1], parameters[2], parameters[3], parameters[4])\n",
    "            classifying_network.load_state_dict(torch.load('../../1_Data/4_K-fold_combined/'+str(iteration)+'/1_2_2_1_256_5e-05/optimal_weights'))\n",
    "            classifying_network.freeze_classification()\n",
    "\n",
    "            # Optimizer    \n",
    "            optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, classifying_network.parameters()), lr=learning_rate)\n",
    "            #optimizer = torch.optim.Adam(params=classifying_network.parameters(), lr=0.00005)\n",
    "\n",
    "            # Create output folder\n",
    "            data_folder = folder+'/'+str(parameters[0])+'_'+str(parameters[1])+'_'+str(parameters[2])+'_'+str(parameters[3])+'_'+str(parameters[4])+'_'+str(learning_rate)+'/'\n",
    "            os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "            # Train & save weights\n",
    "            train_loss, dev_train_loss, test_loss, train_f1, dev_train_f1, test_f1 = train_network(n_epochs, dataloaders_chiasm, classifying_network, optimizer, criterion, device, data_folder)\n",
    "\n",
    "            # Save losses\n",
    "            with open(data_folder+'train_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(train_loss, f)\n",
    "\n",
    "            with open(data_folder+'dev_train_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(dev_train_loss, f)\n",
    "\n",
    "            with open(data_folder+'test_loss.pkl', 'wb') as f:\n",
    "                pickle.dump(test_loss, f)\n",
    "\n",
    "            with open(data_folder+'train_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(train_f1, f)\n",
    "\n",
    "            with open(data_folder+'dev_train_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(dev_train_f1, f)\n",
    "\n",
    "            with open(data_folder+'test_f1.pkl', 'wb') as f:\n",
    "                pickle.dump(test_f1, f)\n",
    "                \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
